{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì˜¤ì°¨ì œê³±í•©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E=\\frac{1}{2} \\sum_{k}(y_{k}-t_{k})^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y, t):\n",
    "    #### ì½”ë“œ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "assert sum_squares_error(np.array(y), np.array(t)) == 0.09750000000000003, 'ì˜¤ë‹µì…ë‹ˆë‹¤.'\n",
    "\n",
    "y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "assert sum_squares_error(np.array(y), np.array(t)) == 0.5975, 'ì˜¤ë‹µì…ë‹ˆë‹¤.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E=-\\sum_{k} t_{k}\\log y_{k}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    #### ì½”ë“œ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "assert cross_entropy_error(np.array(y), np.array(t)) == 0.510825457099338, 'ì˜¤ë‹µì…ë‹ˆë‹¤.'\n",
    "\n",
    "y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "assert cross_entropy_error(np.array(y), np.array(t)) == 2.302584092994546, 'ì˜¤ë‹µì…ë‹ˆë‹¤.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë°°ì¹˜ìš© êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    #### ì½”ë“œ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [0,0,1,0,0,0,0,0,0,0]\n",
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "assert cross_entropy_error(np.array(y), np.array(t)) == 0.510825457099338, 'ì˜¤ë‹µì…ë‹ˆë‹¤.'\n",
    "\n",
    "y = [0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "assert cross_entropy_error(np.array(y), np.array(t)) == 2.302584092994546, 'ì˜¤ë‹µì…ë‹ˆë‹¤.'\n",
    "\n",
    "t = [2]\n",
    "y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "assert cross_entropy_error(np.array(y), np.array(t)) == 0.510825457099338, 'tëŠ” ì›-í•« ë²¡í„°ê°€ ì•„ë‹ ìˆ˜ ìˆìŠµë‹ˆë‹¤.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ebc9d0680a4e4eb26241d6de924a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718590564b514d468659e8ceb7b813eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a7497dee69f4c9f834f01040fe98876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff690b9c2759471db2b67a7cce8d664f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618000d7e8814e8d8f2a09e5a537ae85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "context = \"\"\"\n",
    "ğŸ¤— Transformers is backed by the three most popular deep learning libraries â€” Jax, PyTorch, and TensorFlow â€” with a seamless integration\n",
    "between them. It's straightforward to train your models with one before loading them for inference with the other.\n",
    "\"\"\"\n",
    "question = \"Which deep learning libraries back ğŸ¤— Transformers?\"\n",
    "\n",
    "model_checkpoint = \"distilbert-base-cased-distilled-squad\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "sequence_ids = inputs.sequence_ids()\n",
    "# Mask everything apart from the tokens of the context\n",
    "mask = [i != 1 for i in sequence_ids]\n",
    "# Unmask the [CLS] token\n",
    "mask[0] = False\n",
    "mask = torch.tensor(mask)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False,  True]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([i != 1 for i in sequence_ids])[None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìˆ˜ì¹˜ ë¯¸ë¶„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac {df(x)} {dx} =\\lim_{h \\to 0} \\frac{f(x+h)-f(x-h)}{2h}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    #### ì½”ë“œ ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "assert numerical_diff(function_1, 5) == 0.1999999999990898, 'ë¯¸ë¶„ í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.'\n",
    "assert numerical_diff(function_1, 10) == 0.2999999999986347, 'ë¯¸ë¶„ í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê¸°ìš¸ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x:np.ndarray):\n",
    "    h = 1e-4\n",
    "    #### ì‹œì‘ ####\n",
    "\n",
    "    #### ë ####\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function_2\n",
    "$$f(x_{0}, x_{1})=x^{2}_{0} + x^{2}_{1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "y = numerical_gradient(function_2, np.array([3.0, 4.0]))\n",
    "assert np.allclose(y,  np.array([6., 8.])), 'ì˜¤ë‹µì…ë‹ˆë‹¤.'\n",
    "\n",
    "y = numerical_gradient(function_2, np.array([0.0, 2.0]))\n",
    "assert np.allclose(y,  np.array([0., 4.])), 'ì˜¤ë‹µì…ë‹ˆë‹¤.'\n",
    "\n",
    "y = numerical_gradient(function_2, np.array([3.0, 0.0]))\n",
    "assert np.allclose(y,  np.array([6., 0.])), 'ì˜¤ë‹µì…ë‹ˆë‹¤.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
