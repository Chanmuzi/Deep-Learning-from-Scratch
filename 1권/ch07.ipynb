{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 합성곱/풀링 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4차원 배열"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1, 28, 28)\n",
      "10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(10, 1, 28, 28) # 데이터 수, 채널, 높이, 너비\n",
    "print(x.shape) \n",
    "print(x.shape[0])\n",
    "print(x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im2col로 데이터 전개하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2*pad - filter_w) // stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant') ## height, width에만 padding 추가\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            ## col 배열의 (0, 0) 위치에 해당하는 모든 '윈도우'에 중복되어 저장\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "    \n",
    "    ## N, out_h, out_w, C, h, w -> (N * out_h * out_w, C * h * w)\n",
    "    col = col.transpose(0, 4, 5, 1, 2 ,3).reshape(N * out_h * out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.random.rand(6, 3, 2, 2)\n",
    "pad = 1\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 이미지 데이터:\n",
      "[[[[ 0  1  2  3]\n",
      "   [ 4  5  6  7]\n",
      "   [ 8  9 10 11]\n",
      "   [12 13 14 15]]]]\n",
      "[[[[[[0. 0.]\n",
      "     [0. 0.]]\n",
      "\n",
      "    [[0. 0.]\n",
      "     [0. 0.]]]\n",
      "\n",
      "\n",
      "   [[[0. 0.]\n",
      "     [0. 0.]]\n",
      "\n",
      "    [[0. 0.]\n",
      "     [0. 0.]]]]]]\n",
      "row: 0, col: 0\n",
      "[[[[[[ 0.  2.]\n",
      "     [ 8. 10.]]\n",
      "\n",
      "    [[ 0.  0.]\n",
      "     [ 0.  0.]]]\n",
      "\n",
      "\n",
      "   [[[ 0.  0.]\n",
      "     [ 0.  0.]]\n",
      "\n",
      "    [[ 0.  0.]\n",
      "     [ 0.  0.]]]]]]\n",
      "row: 0, col: 1\n",
      "[[[[[[ 0.  2.]\n",
      "     [ 8. 10.]]\n",
      "\n",
      "    [[ 1.  3.]\n",
      "     [ 9. 11.]]]\n",
      "\n",
      "\n",
      "   [[[ 0.  0.]\n",
      "     [ 0.  0.]]\n",
      "\n",
      "    [[ 0.  0.]\n",
      "     [ 0.  0.]]]]]]\n",
      "row: 1, col: 0\n",
      "[[[[[[ 0.  2.]\n",
      "     [ 8. 10.]]\n",
      "\n",
      "    [[ 1.  3.]\n",
      "     [ 9. 11.]]]\n",
      "\n",
      "\n",
      "   [[[ 4.  6.]\n",
      "     [12. 14.]]\n",
      "\n",
      "    [[ 0.  0.]\n",
      "     [ 0.  0.]]]]]]\n",
      "row: 1, col: 1\n",
      "[[[[[[ 0.  2.]\n",
      "     [ 8. 10.]]\n",
      "\n",
      "    [[ 1.  3.]\n",
      "     [ 9. 11.]]]\n",
      "\n",
      "\n",
      "   [[[ 4.  6.]\n",
      "     [12. 14.]]\n",
      "\n",
      "    [[ 5.  7.]\n",
      "     [13. 15.]]]]]]\n",
      "\n",
      "평탄화된 데이터 (col 배열):\n",
      "[[[[[[ 0.  2.]\n",
      "     [ 8. 10.]]\n",
      "\n",
      "    [[ 1.  3.]\n",
      "     [ 9. 11.]]]\n",
      "\n",
      "\n",
      "   [[[ 4.  6.]\n",
      "     [12. 14.]]\n",
      "\n",
      "    [[ 5.  7.]\n",
      "     [13. 15.]]]]]]\n",
      "\n",
      "transpose 배열 (col 배열):\n",
      "[[[[[[ 0.  1.]\n",
      "     [ 4.  5.]]]\n",
      "\n",
      "\n",
      "   [[[ 2.  3.]\n",
      "     [ 6.  7.]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[ 8.  9.]\n",
      "     [12. 13.]]]\n",
      "\n",
      "\n",
      "   [[[10. 11.]\n",
      "     [14. 15.]]]]]]\n",
      "\n",
      "최종 2차원 배열 (col 배열):\n",
      "[[ 0.  1.  4.  5.]\n",
      " [ 2.  3.  6.  7.]\n",
      " [ 8.  9. 12. 13.]\n",
      " [10. 11. 14. 15.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 예시 데이터 설정\n",
    "N, C, H, W = 1, 1, 4, 4  # 1개의 이미지, 1개의 채널, 4x4 크기\n",
    "filter_h, filter_w, stride, pad = 2, 2, 2, 0  # 2x2 필터, 스트라이드 2, 패딩 0\n",
    "\n",
    "# 4x4 크기의 이미지 데이터 생성 (값은 행과 열 인덱스의 합으로 설정)\n",
    "input_data = np.array([[[[i*4 + j for j in range(W)] for i in range(H)]]])\n",
    "print(\"원본 이미지 데이터:\")\n",
    "print(input_data)\n",
    "\n",
    "# 출력 데이터의 높이와 너비 계산\n",
    "out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "# 패딩 적용 (이 경우에는 패딩이 0이므로 원본 데이터 그대로 사용)\n",
    "img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "\n",
    "# 2차원 배열 초기화\n",
    "col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "print(col)\n",
    "# 이미지 데이터의 평탄화\n",
    "for y in range(filter_h):\n",
    "    y_max = y + stride*out_h\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + stride*out_w\n",
    "        col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "        print(f\"row: {y}, col: {x}\")\n",
    "        print(col)\n",
    "\n",
    "print(\"\\n평탄화된 데이터 (col 배열):\")\n",
    "print(col)\n",
    "\n",
    "# 차원 재배열 및 최종 2차원 배열 생성\n",
    "col = col.transpose(0, 4, 5, 1, 2, 3)\n",
    "print(\"\\ntranspose 배열 (col 배열):\")\n",
    "print(col)\n",
    "\n",
    "col = col.reshape(N*out_h*out_w, -1)\n",
    "\n",
    "print(\"\\n최종 2차원 배열 (col 배열):\")\n",
    "print(col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "col2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, W, H = input_shape\n",
    "    out_h = (H + 2*pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2*pad - filter_w) // stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x +stride * out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:pad+H, pad:pad+W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose 후의 col 배열:\n",
      "[[[[[[ 0  2]\n",
      "     [ 8 10]]\n",
      "\n",
      "    [[ 1  3]\n",
      "     [ 9 11]]]\n",
      "\n",
      "\n",
      "   [[[ 4  6]\n",
      "     [12 14]]\n",
      "\n",
      "    [[ 5  7]\n",
      "     [13 15]]]]]]\n",
      "초기 이미지 배열:\n",
      "[[[[0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]\n",
      "   [0. 0. 0. 0. 0.]]]]\n",
      "y: 0, x: 0, img 배열 업데이트 후:\n",
      "[[[[ 0.  0.  2.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.]\n",
      "   [ 8.  0. 10.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.]]]]\n",
      "y: 0, x: 1, img 배열 업데이트 후:\n",
      "[[[[ 0.  1.  2.  3.  0.]\n",
      "   [ 0.  0.  0.  0.  0.]\n",
      "   [ 8.  9. 10. 11.  0.]\n",
      "   [ 0.  0.  0.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.]]]]\n",
      "y: 1, x: 0, img 배열 업데이트 후:\n",
      "[[[[ 0.  1.  2.  3.  0.]\n",
      "   [ 4.  0.  6.  0.  0.]\n",
      "   [ 8.  9. 10. 11.  0.]\n",
      "   [12.  0. 14.  0.  0.]\n",
      "   [ 0.  0.  0.  0.  0.]]]]\n",
      "y: 1, x: 1, img 배열 업데이트 후:\n",
      "[[[[ 0.  1.  2.  3.  0.]\n",
      "   [ 4.  5.  6.  7.  0.]\n",
      "   [ 8.  9. 10. 11.  0.]\n",
      "   [12. 13. 14. 15.  0.]\n",
      "   [ 0.  0.  0.  0.  0.]]]]\n",
      "최종 이미지:\n",
      "[[[[ 0.  1.  2.  3.]\n",
      "   [ 4.  5.  6.  7.]\n",
      "   [ 8.  9. 10. 11.]\n",
      "   [12. 13. 14. 15.]]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 입력 데이터 설정\n",
    "N, C, H, W = 1, 1, 4, 4  # 1개의 이미지, 1개의 채널, 4x4 크기\n",
    "filter_h, filter_w, stride, pad = 2, 2, 2, 0  # 2x2 필터, 스트라이드 2, 패딩 0\n",
    "\n",
    "# im2col로 변환된 데이터 (예시)\n",
    "col = np.array([[[[ 0,  1], [ 4,  5]],\n",
    "                 [[ 2,  3], [ 6,  7]]],\n",
    "                [[[ 8,  9], [12, 13]],\n",
    "                 [[10, 11], [14, 15]]]])\n",
    "\n",
    "# 출력 데이터의 높이와 너비 계산\n",
    "out_h = (H + 2*pad - filter_h) // stride + 1\n",
    "out_w = (W + 2*pad - filter_w) // stride + 1\n",
    "\n",
    "# 평탄화된 데이터의 차원 재배열\n",
    "col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "print(\"Transpose 후의 col 배열:\")\n",
    "print(col)\n",
    "\n",
    "# 초기 이미지 배열 생성\n",
    "img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "print(\"초기 이미지 배열:\")\n",
    "print(img)\n",
    "\n",
    "# col에서 이미지로 데이터 복원\n",
    "for y in range(filter_h):\n",
    "    y_max = y + stride * out_h\n",
    "    for x in range(filter_w):\n",
    "        x_max = x + stride * out_w\n",
    "        ## 각 필터를 통해 획득된 값을 누적하여 이미지를 복원. 중복되는 픽셀이 존재하는 경우 반드시 += 연산자를 사용해야 함\n",
    "        img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "        print(f\"y: {y}, x: {x}, img 배열 업데이트 후:\")\n",
    "        print(img)\n",
    "\n",
    "# 최종 이미지 추출\n",
    "img = img[:, :, pad:pad+H, pad:pad+W]\n",
    "print(\"최종 이미지:\")\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7) # 데이터, 채널, 높이, 너비\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) ## filter_h * filter_w * channel 수만큼의 feature 생성\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution: \n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.x = None\n",
    "        self.col = None\n",
    "        self.col_W = None\n",
    "\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape # 필터 개수, 채널, 높이, 너비\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int((H + 2*self.pad - FH) / self.stride + 1)\n",
    "        out_w = int((W + 2*self.pad - FW) / self.stride + 1)\n",
    "\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad) ## (N * H * W, C * FH * FW)\n",
    "        col_W = self.W.reshape(FN, -1).T # 필터 펼치기 ## (FN, C * FH * FW).T -> (C * FH * FW, FN)\n",
    "        \n",
    "        out = np.dot(col, col_W) + self.b ## (C * FH * FW, FN)\n",
    "\n",
    "        ## (FN, FH, FW, C) -> (FN, C, FH, FW)\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "\n",
    "        self.x = x\n",
    "        self.col = col\n",
    "        self.col_W = col_W\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
    "\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        self.dW = np.dot(self.col.T, dout)\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
    "\n",
    "        dcol = np.dot(dout, self.col_W.T)\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
    "\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "입력 데이터 x:\n",
      "[[[[0.15095727 0.82056136 0.11493924 0.75676101]\n",
      "   [0.94081566 0.34925049 0.94565707 0.20490969]\n",
      "   [0.90130254 0.21916467 0.92908738 0.18050672]\n",
      "   [0.00481105 0.26711582 0.35202711 0.17179169]]]]\n",
      "\n",
      "필터 가중치 W:\n",
      "[[[[0.77842886 0.58721771 0.20868072]\n",
      "   [0.10710841 0.34030623 0.3961517 ]\n",
      "   [0.51695162 0.39108652 0.97186053]]]]\n",
      "\n",
      "편향 b:\n",
      "[0.46235792]\n",
      "\n",
      "컨볼루션 결과 out:\n",
      "[[[[3.13453175 2.41899677]\n",
      "   [2.58539591 2.18616509]]]]\n"
     ]
    }
   ],
   "source": [
    "## 예시 입력 데이터, 필터 가중치, 편향 설정\n",
    "x = np.random.rand(1, 1, 4, 4) # 임의의 4x4 크기의 이미지 데이터\n",
    "weights = np.random.rand(1, 1, 3, 3) # 3x3 크기의 필터\n",
    "b = np.random.rand(1) # 편향\n",
    "stride = 1\n",
    "pad = 0\n",
    "# Convolution의 forward 함수 대신 독립된 함수로 변환\n",
    "\n",
    "FN, C, FH, FW = weights.shape # 필터 개수, 채널 수, 필터 높이, 필터 너비 -> 1, 1, 3, 3\n",
    "N, C, H, W = x.shape # 이미지 수, 채널 수, 높이, 너비 -> 1, 1, 4, 4\n",
    "out_h = int((H + 2*pad - FH) / stride + 1) # 출력 데이터의 높이 -> 2\n",
    "out_w = int((W + 2*pad - FW) / stride + 1) # 출력 데이터의 너비 -> 2\n",
    "\n",
    "# im2col 함수를 사용하여 입력 데이터를 2차원 배열로 변환\n",
    "col = im2col(x, FH, FW, stride, pad) # (4, 9) = (이미지 수 * 출력 높이 * 출력 너비, 필터 높이 * 필터 너비 * 채널 수)\n",
    "# 필터를 2차원 배열로 변환\n",
    "col_weights = weights.reshape(-1, FN)\n",
    "# col_weights = weights.reshape(FN, -1).T # (1, 9).T = (FN, C * FH * FH).T = (필터 개수, 채널 수 * 필터 높이 * 필터 너비).T\n",
    "\n",
    "# 컨볼루션 연산 수행: (4, 9) * (9, 1) = (4, 1) = (이미지 수 * 출력 높이 * 출력 너비, 필터 개수)\n",
    "out = np.dot(col, col_weights) + b\n",
    "print(out.shape)\n",
    "\n",
    "# 출력 데이터를 적절한 형태로 재배열\n",
    "out = out.reshape(N, out_h, out_w, -1) # (이미지 수, 출력 높이, 출력 너비, 필터 개수)\n",
    "\n",
    "out = out.transpose(0, 3, 1, 2) # (이미지 수, 필터 개수, 출력 높이, 출력 너비)\n",
    "\n",
    "print(\"입력 데이터 x:\")\n",
    "print(x)\n",
    "print(\"\\n필터 가중치 W:\")\n",
    "print(weights)\n",
    "print(\"\\n편향 b:\")\n",
    "print(b)\n",
    "print(\"\\n컨볼루션 결과 out:\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "풀링 계층 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "\n",
    "        self.x = None\n",
    "        self.arg_max = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad) ## (이미지 수 * 출력 높이 * 출력 너비, 채널 수 * 필터 높이 * 필터 너비)\n",
    "        col = col.reshape(-1, self.pool_h * self.pool_w) ## (나머지, 필터 높이 * 필터 너비) => 필터 영역이 각 행을 이룰 수 있도록 하여 행의 최댓값을 추출\n",
    "\n",
    "        arg_max = np.argmax(col, axis=1)\n",
    "        out = np.max(col, axis=1) ## 각 행의 최댓값을 구해야하므로 axis=1이 됨. 여기서는 (필터 높이 * 필터 너비), 필터 영역의 최댓값을 구하게 됨\n",
    "\n",
    "        ## 필터 사이즈 * 필터 개수만큼 pooling 결과가 늘어지므로 reshape을 진행 -> 이후 transpose를 통해 (N, C, out_h, out_w) 모양을 맞춰줌\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n",
    "\n",
    "        self.x = x\n",
    "        self.arg_max = arg_max\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = dout.transpose(0, 2, 3, 1)\n",
    "        \n",
    "        pool_size = self.pool_h * self.pool_w\n",
    "        dmax = np.zeros((dout.size, pool_size))\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \n",
    "        \n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실험 포인트!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "\n",
    "        self.x = None\n",
    "        self.original_x_shape = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        dx = dx.reshape(*self.original_x_shape)\n",
    "        return dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T\n",
    "    \n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    return - np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답이 원-핫 인코딩인 경우\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}, \n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1] ## height & width 가 같은 값일 것이라고 가정한 듯함\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2)) ## 2x2 pooling filter\n",
    "\n",
    "        self.params = {}\n",
    "        ## 첫 번째 입력 x는 (이미지 수, 채널 수, 높이, 너비) = (N, C, H, W). 이것과 convolution 할 W -> (FN, C, FH, FW)\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        ## bias는 그냥 더해주면 됨\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    def save_params(self, file_name='params.pkl'):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name='params.pkl'):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "        \n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2*pad - filter_w) // stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad,pad), (pad,pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            col[:,:,y,x,:,:] = img[:,:,y:y_max:stride,x:x_max:stride]\n",
    "    \n",
    "    ## (이미지 수, 출력 높이, 출력 너비, 필터 수, 필터 높이, 필터 너비) => 필터에 찍힐 대상이 열벡터를 이루게 됨\n",
    "    col = col.transpose(0,4,5,1,2,3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299470088318002\n",
      "=== epoch:1, train acc:0.261, test acc:0.301 ===\n",
      "train loss:2.2968717810557426\n",
      "train loss:2.2911460335199596\n",
      "train loss:2.286943628019342\n",
      "train loss:2.270870787721301\n",
      "train loss:2.263628511727829\n",
      "train loss:2.2587571900330605\n",
      "train loss:2.227721476548093\n",
      "train loss:2.2054915245988234\n",
      "train loss:2.167175432320232\n",
      "train loss:2.1312321835158134\n",
      "train loss:2.0766776790048187\n",
      "train loss:2.0238304241137413\n",
      "train loss:1.9985354043848154\n",
      "train loss:1.948628439023496\n",
      "train loss:1.9025588852130861\n",
      "train loss:1.723575320533841\n",
      "train loss:1.7243269030134993\n",
      "train loss:1.7294891286194807\n",
      "train loss:1.54955609053461\n",
      "train loss:1.4571246387644823\n",
      "train loss:1.325213718968883\n",
      "train loss:1.2943489066537006\n",
      "train loss:1.3238180613912696\n",
      "train loss:1.0209838794589297\n",
      "train loss:1.0062980819559\n",
      "train loss:1.035069038836398\n",
      "train loss:0.9347359702212544\n",
      "train loss:0.9537026064183861\n",
      "train loss:0.9297879451079092\n",
      "train loss:0.7621558309138307\n",
      "train loss:0.7929923069507242\n",
      "train loss:0.734742470934545\n",
      "train loss:0.7496254599759198\n",
      "train loss:0.703963517285704\n",
      "train loss:0.6979627853233272\n",
      "train loss:0.7064582928530239\n",
      "train loss:0.5794284047655567\n",
      "train loss:0.6081467412088276\n",
      "train loss:0.5788802451432798\n",
      "train loss:0.5596402971307022\n",
      "train loss:0.6040252473053617\n",
      "train loss:0.5878714663190803\n",
      "train loss:0.6098799125922502\n",
      "train loss:0.5126241478540591\n",
      "train loss:0.5407870196771762\n",
      "train loss:0.5158377074787343\n",
      "train loss:0.3878785056604355\n",
      "train loss:0.394247790716621\n",
      "train loss:0.6455741103803089\n",
      "train loss:0.5440072072936077\n",
      "=== epoch:2, train acc:0.824, test acc:0.799 ===\n",
      "train loss:0.6111499963549251\n",
      "train loss:0.4487982355125068\n",
      "train loss:0.5234913365988948\n",
      "train loss:0.44986856514255735\n",
      "train loss:0.4166280628540373\n",
      "train loss:0.5699267575013119\n",
      "train loss:0.3128371090750896\n",
      "train loss:0.4046281192393322\n",
      "train loss:0.6174083260935843\n",
      "train loss:0.5244956171432574\n",
      "train loss:0.29588835101584143\n",
      "train loss:0.3476225457812064\n",
      "train loss:0.43781514456940324\n",
      "train loss:0.432810046150152\n",
      "train loss:0.546941069503363\n",
      "train loss:0.5408248867694225\n",
      "train loss:0.32094853690672437\n",
      "train loss:0.5314082808497209\n",
      "train loss:0.3641389421041016\n",
      "train loss:0.42541654981462196\n",
      "train loss:0.4449488967067343\n",
      "train loss:0.2944664052811196\n",
      "train loss:0.38865921455333485\n",
      "train loss:0.45123893205424276\n",
      "train loss:0.4972521245665215\n",
      "train loss:0.4512128932705851\n",
      "train loss:0.3906326115600697\n",
      "train loss:0.4853392766567845\n",
      "train loss:0.31296130466995226\n",
      "train loss:0.3129976155024762\n",
      "train loss:0.41640868406013465\n",
      "train loss:0.34992564610093047\n",
      "train loss:0.31863522667570787\n",
      "train loss:0.20290037950602438\n",
      "train loss:0.3545143683674533\n",
      "train loss:0.3665554620038789\n",
      "train loss:0.3245342451463731\n",
      "train loss:0.41093278081017054\n",
      "train loss:0.3734151910882359\n",
      "train loss:0.4431509010827378\n",
      "train loss:0.3525447108695605\n",
      "train loss:0.5120202390681865\n",
      "train loss:0.27179299177387284\n",
      "train loss:0.24773661646582038\n",
      "train loss:0.3767772418388574\n",
      "train loss:0.33112761726021245\n",
      "train loss:0.3507161565638303\n",
      "train loss:0.36474731638883834\n",
      "train loss:0.4051713174162608\n",
      "train loss:0.261259852111966\n",
      "=== epoch:3, train acc:0.881, test acc:0.866 ===\n",
      "train loss:0.4041076368034064\n",
      "train loss:0.2712957350777893\n",
      "train loss:0.2543633524666619\n",
      "train loss:0.4479588094136787\n",
      "train loss:0.3544095479508396\n",
      "train loss:0.25999110517356117\n",
      "train loss:0.4501893905032918\n",
      "train loss:0.39427908891444036\n",
      "train loss:0.3860333588832874\n",
      "train loss:0.5240643988006485\n",
      "train loss:0.35099555511971725\n",
      "train loss:0.2915149883189765\n",
      "train loss:0.3063605917542703\n",
      "train loss:0.3601282481038329\n",
      "train loss:0.38363811240806095\n",
      "train loss:0.24410299232264537\n",
      "train loss:0.1911695860080747\n",
      "train loss:0.35686763452274\n",
      "train loss:0.33447944488578457\n",
      "train loss:0.2661811246356753\n",
      "train loss:0.25852628292531943\n",
      "train loss:0.28339408688918627\n",
      "train loss:0.30987273721691655\n",
      "train loss:0.4825043419662607\n",
      "train loss:0.40429696315512914\n",
      "train loss:0.3324016373453737\n",
      "train loss:0.23760826618488357\n",
      "train loss:0.24905520714831694\n",
      "train loss:0.23122648010185207\n",
      "train loss:0.22795530245436932\n",
      "train loss:0.27311163624856255\n",
      "train loss:0.22651925553308205\n",
      "train loss:0.33327127492721736\n",
      "train loss:0.2737623886664576\n",
      "train loss:0.3577030331371961\n",
      "train loss:0.4175358157175416\n",
      "train loss:0.18694469623982074\n",
      "train loss:0.29724270122797314\n",
      "train loss:0.1883148171767608\n",
      "train loss:0.2649517679148817\n",
      "train loss:0.3484393408073359\n",
      "train loss:0.2936355188808235\n",
      "train loss:0.20622363649507963\n",
      "train loss:0.15538405335583785\n",
      "train loss:0.3366558276298655\n",
      "train loss:0.35087958608235836\n",
      "train loss:0.2758688923673306\n",
      "train loss:0.1837300391058484\n",
      "train loss:0.22050910019428752\n",
      "train loss:0.24520777607514954\n",
      "=== epoch:4, train acc:0.916, test acc:0.889 ===\n",
      "train loss:0.23404505529681824\n",
      "train loss:0.3432827203291303\n",
      "train loss:0.23417732740881472\n",
      "train loss:0.19029322354139772\n",
      "train loss:0.4730475554534613\n",
      "train loss:0.3804544129269096\n",
      "train loss:0.2590121045120261\n",
      "train loss:0.4758045821905504\n",
      "train loss:0.304108101078258\n",
      "train loss:0.20256115749845727\n",
      "train loss:0.22765959128630453\n",
      "train loss:0.32757251229730605\n",
      "train loss:0.4322503144496889\n",
      "train loss:0.32567010150807296\n",
      "train loss:0.26799207874726055\n",
      "train loss:0.2544272224969408\n",
      "train loss:0.15696827902755384\n",
      "train loss:0.22146478619836432\n",
      "train loss:0.17017680723309334\n",
      "train loss:0.20714742188868232\n",
      "train loss:0.32013536185362645\n",
      "train loss:0.27615660360613054\n",
      "train loss:0.2921250502384632\n",
      "train loss:0.26247408100311065\n",
      "train loss:0.3463005140797364\n",
      "train loss:0.25357055555230007\n",
      "train loss:0.24448085004928402\n",
      "train loss:0.19795042624863554\n",
      "train loss:0.22498442028537688\n",
      "train loss:0.18616971464155296\n",
      "train loss:0.20950567850852828\n",
      "train loss:0.21637269163015252\n",
      "train loss:0.1414858256006693\n",
      "train loss:0.21468582065398212\n",
      "train loss:0.20509133074416536\n",
      "train loss:0.24277695596766072\n",
      "train loss:0.25397510698404113\n",
      "train loss:0.29010136694242394\n",
      "train loss:0.2626059993902581\n",
      "train loss:0.23185800976221912\n",
      "train loss:0.17658274055675155\n",
      "train loss:0.2208843063233257\n",
      "train loss:0.31563078511246045\n",
      "train loss:0.2818334164705459\n",
      "train loss:0.2642887373556107\n",
      "train loss:0.17536120733498323\n",
      "train loss:0.2098257890174166\n",
      "train loss:0.3219484232262091\n",
      "train loss:0.15560541670270256\n",
      "train loss:0.2420026531056898\n",
      "=== epoch:5, train acc:0.918, test acc:0.901 ===\n",
      "train loss:0.2490193153013694\n",
      "train loss:0.1089196660067521\n",
      "train loss:0.18158902078561479\n",
      "train loss:0.1288369409272765\n",
      "train loss:0.1576443268447824\n",
      "train loss:0.14762640605522612\n",
      "train loss:0.33746308863057856\n",
      "train loss:0.22006915846556488\n",
      "train loss:0.13457329604600102\n",
      "train loss:0.30175227650639097\n",
      "train loss:0.12089689987626193\n",
      "train loss:0.34815298145065066\n",
      "train loss:0.15769884290644884\n",
      "train loss:0.19443065829023012\n",
      "train loss:0.20640161850566038\n",
      "train loss:0.2618234229864755\n",
      "train loss:0.2752924361570796\n",
      "train loss:0.29632122529877\n",
      "train loss:0.17878733512443104\n",
      "train loss:0.14095332636246177\n",
      "train loss:0.16521237832125502\n",
      "train loss:0.15011464350476014\n",
      "train loss:0.2404003511148188\n",
      "train loss:0.20118779568970005\n",
      "train loss:0.17078544590875183\n",
      "train loss:0.14984282381534106\n",
      "train loss:0.22213514702748804\n",
      "train loss:0.11569567528255707\n",
      "train loss:0.15017320535892373\n",
      "train loss:0.237928732860929\n",
      "train loss:0.2679339908218443\n",
      "train loss:0.17803987232612573\n",
      "train loss:0.34760190482885633\n",
      "train loss:0.30669566581007923\n",
      "train loss:0.1706842573720175\n",
      "train loss:0.1713509340242239\n",
      "train loss:0.26957397265559946\n",
      "train loss:0.19624245147348643\n",
      "train loss:0.18561481746604527\n",
      "train loss:0.12448396441949537\n",
      "train loss:0.12187468080323444\n",
      "train loss:0.16021181002805612\n",
      "train loss:0.09360793765656983\n",
      "train loss:0.1960180937923038\n",
      "train loss:0.17982689327265475\n",
      "train loss:0.25421164204596497\n",
      "train loss:0.07810607182636053\n",
      "train loss:0.11564841070001718\n",
      "train loss:0.09443072416630598\n",
      "train loss:0.21563627084406037\n",
      "=== epoch:6, train acc:0.934, test acc:0.906 ===\n",
      "train loss:0.1267604080074861\n",
      "train loss:0.233408514303181\n",
      "train loss:0.13245615005252168\n",
      "train loss:0.264538182621383\n",
      "train loss:0.29903974637467384\n",
      "train loss:0.2181487960863696\n",
      "train loss:0.22193340719206886\n",
      "train loss:0.09966606708264272\n",
      "train loss:0.18835589932298547\n",
      "train loss:0.17250824438872162\n",
      "train loss:0.14491410588542136\n",
      "train loss:0.12861404022868436\n",
      "train loss:0.16801063755545617\n",
      "train loss:0.29670634731693496\n",
      "train loss:0.20745495974452427\n",
      "train loss:0.13034381241796128\n",
      "train loss:0.24212535595445112\n",
      "train loss:0.2522115087886146\n",
      "train loss:0.15354720576460484\n",
      "train loss:0.14912074330658143\n",
      "train loss:0.20767094409821493\n",
      "train loss:0.20442979281632095\n",
      "train loss:0.20449772528063043\n",
      "train loss:0.2203179854216823\n",
      "train loss:0.194483967165896\n",
      "train loss:0.2144718827399986\n",
      "train loss:0.1579835701021468\n",
      "train loss:0.23771113343104552\n",
      "train loss:0.1401786180925073\n",
      "train loss:0.1504357573598959\n",
      "train loss:0.06372384801415147\n",
      "train loss:0.214079908372185\n",
      "train loss:0.24470915713811703\n",
      "train loss:0.14079958491606478\n",
      "train loss:0.17360427033989773\n",
      "train loss:0.2085622559547865\n",
      "train loss:0.2203677279863072\n",
      "train loss:0.11797800014888088\n",
      "train loss:0.1435378822316536\n",
      "train loss:0.16221328551283612\n",
      "train loss:0.16809658415667372\n",
      "train loss:0.14228339755328623\n",
      "train loss:0.13308122969990943\n",
      "train loss:0.1971067607648768\n",
      "train loss:0.13499828953174936\n",
      "train loss:0.1326514093298495\n",
      "train loss:0.11123261680850645\n",
      "train loss:0.24906675036552353\n",
      "train loss:0.07919406890608836\n",
      "train loss:0.12231855014260239\n",
      "=== epoch:7, train acc:0.935, test acc:0.924 ===\n",
      "train loss:0.2013692065123432\n",
      "train loss:0.2623045783644221\n",
      "train loss:0.08165451694720074\n",
      "train loss:0.17776949032647887\n",
      "train loss:0.19626477833411204\n",
      "train loss:0.10116836712352527\n",
      "train loss:0.2926750017961167\n",
      "train loss:0.051071291274655824\n",
      "train loss:0.23309042220807638\n",
      "train loss:0.07018856988974567\n",
      "train loss:0.10271862440102614\n",
      "train loss:0.1819853871022599\n",
      "train loss:0.12467876714404542\n",
      "train loss:0.22329006979388746\n",
      "train loss:0.14437115925053692\n",
      "train loss:0.15125262728753913\n",
      "train loss:0.09629973277292458\n",
      "train loss:0.2384524692106735\n",
      "train loss:0.10520922235422027\n",
      "train loss:0.2194497840578854\n",
      "train loss:0.07817953825649498\n",
      "train loss:0.2574259175021664\n",
      "train loss:0.13608861400087774\n",
      "train loss:0.13023963201205904\n",
      "train loss:0.10418604864895652\n",
      "train loss:0.16375820639538044\n",
      "train loss:0.2369221652213204\n",
      "train loss:0.3263120532306361\n",
      "train loss:0.10540622605782884\n",
      "train loss:0.1236359673119354\n",
      "train loss:0.06361043815022736\n",
      "train loss:0.07479311129237881\n",
      "train loss:0.12848587225230987\n",
      "train loss:0.14158967258079425\n",
      "train loss:0.16484531311650538\n",
      "train loss:0.12345557966638442\n",
      "train loss:0.14379032104079312\n",
      "train loss:0.17486215400669555\n",
      "train loss:0.09544200997516458\n",
      "train loss:0.14579513637646616\n",
      "train loss:0.1328852504077801\n",
      "train loss:0.20894305976093158\n",
      "train loss:0.12832967133492734\n",
      "train loss:0.16029950781484864\n",
      "train loss:0.09928240744347303\n",
      "train loss:0.11586815620284208\n",
      "train loss:0.1819109767461908\n",
      "train loss:0.06275325451637556\n",
      "train loss:0.19609858261942914\n",
      "train loss:0.11279815765456398\n",
      "=== epoch:8, train acc:0.955, test acc:0.92 ===\n",
      "train loss:0.17344778263707503\n",
      "train loss:0.05627225619229481\n",
      "train loss:0.1780988713225437\n",
      "train loss:0.07535574956168924\n",
      "train loss:0.33079320142116736\n",
      "train loss:0.11374706712322034\n",
      "train loss:0.15778215186410544\n",
      "train loss:0.13604775424590293\n",
      "train loss:0.09794068919771513\n",
      "train loss:0.08797198777701407\n",
      "train loss:0.1146749679413889\n",
      "train loss:0.12299060228611042\n",
      "train loss:0.13934846344220822\n",
      "train loss:0.20667640319630595\n",
      "train loss:0.1398879093547299\n",
      "train loss:0.07075226048092251\n",
      "train loss:0.1962851414079772\n",
      "train loss:0.07961261150828915\n",
      "train loss:0.09255973921641436\n",
      "train loss:0.16198099683545567\n",
      "train loss:0.1780686675982499\n",
      "train loss:0.13307309682103002\n",
      "train loss:0.1011038399836648\n",
      "train loss:0.11899857381716872\n",
      "train loss:0.10646454192576049\n",
      "train loss:0.1674873095058006\n",
      "train loss:0.17501429824796336\n",
      "train loss:0.163848211415919\n",
      "train loss:0.14960866967515363\n",
      "train loss:0.07906728346637834\n",
      "train loss:0.07332992861938092\n",
      "train loss:0.20567278642528003\n",
      "train loss:0.08637828895766424\n",
      "train loss:0.060592686829554765\n",
      "train loss:0.1572249567453075\n",
      "train loss:0.19652285652439366\n",
      "train loss:0.12276063448106606\n",
      "train loss:0.10650549302089711\n",
      "train loss:0.10101655494291613\n",
      "train loss:0.11466414860629146\n",
      "train loss:0.1443460272986974\n",
      "train loss:0.06153065719064691\n",
      "train loss:0.07161799546728824\n",
      "train loss:0.06113307574949161\n",
      "train loss:0.15152936736209813\n",
      "train loss:0.0961105613856473\n",
      "train loss:0.10323495466754701\n",
      "train loss:0.2036680664453482\n",
      "train loss:0.16466938823523983\n",
      "train loss:0.055864542827071186\n",
      "=== epoch:9, train acc:0.949, test acc:0.928 ===\n",
      "train loss:0.08826012517612913\n",
      "train loss:0.040114477453511516\n",
      "train loss:0.11888536382780852\n",
      "train loss:0.14330080314710186\n",
      "train loss:0.19107966887295294\n",
      "train loss:0.1443005162805516\n",
      "train loss:0.18032077473558636\n",
      "train loss:0.12493441607422334\n",
      "train loss:0.06205533417793267\n",
      "train loss:0.08785407991083913\n",
      "train loss:0.10202919219921443\n",
      "train loss:0.18487676678708698\n",
      "train loss:0.11390858004156812\n",
      "train loss:0.12249671042889268\n",
      "train loss:0.11360183796178056\n",
      "train loss:0.08955631918702714\n",
      "train loss:0.09948408373224817\n",
      "train loss:0.07017606655465794\n",
      "train loss:0.15937019656123455\n",
      "train loss:0.1586126280833995\n",
      "train loss:0.0957911520726648\n",
      "train loss:0.1311422528799632\n",
      "train loss:0.04974361449340466\n",
      "train loss:0.06250744822339784\n",
      "train loss:0.12094735074020412\n",
      "train loss:0.11509224183294703\n",
      "train loss:0.09587740629100894\n",
      "train loss:0.05383226445457698\n",
      "train loss:0.11015541986576846\n",
      "train loss:0.18673921936112672\n",
      "train loss:0.07152177065050362\n",
      "train loss:0.06431302594291154\n",
      "train loss:0.05056361237855687\n",
      "train loss:0.16089284624380393\n",
      "train loss:0.11980136917260223\n",
      "train loss:0.11438742716471641\n",
      "train loss:0.0681611024592491\n",
      "train loss:0.17561917518962514\n",
      "train loss:0.07311192965373506\n",
      "train loss:0.15060434535668704\n",
      "train loss:0.08392603325228537\n",
      "train loss:0.0729969001242357\n",
      "train loss:0.10110881761424144\n",
      "train loss:0.08325370394096997\n",
      "train loss:0.10031437128379966\n",
      "train loss:0.10525369298394859\n",
      "train loss:0.1911622491985014\n",
      "train loss:0.060497402187149774\n",
      "train loss:0.17404328578198314\n",
      "train loss:0.08885150503145994\n",
      "=== epoch:10, train acc:0.965, test acc:0.944 ===\n",
      "train loss:0.06367322032002735\n",
      "train loss:0.10445064884165334\n",
      "train loss:0.12618310731135676\n",
      "train loss:0.08399307220694162\n",
      "train loss:0.11000609253371194\n",
      "train loss:0.1531078936889382\n",
      "train loss:0.1262151631072702\n",
      "train loss:0.08076186034313362\n",
      "train loss:0.06047132982560122\n",
      "train loss:0.06482950586739229\n",
      "train loss:0.08508983119484961\n",
      "train loss:0.14390332541266776\n",
      "train loss:0.05025372680047321\n",
      "train loss:0.09446510465971311\n",
      "train loss:0.08150990769510648\n",
      "train loss:0.09023852753272976\n",
      "train loss:0.11252360615749932\n",
      "train loss:0.06458180445690795\n",
      "train loss:0.055173394308253174\n",
      "train loss:0.09784428329115595\n",
      "train loss:0.11790610385220457\n",
      "train loss:0.18584250741902225\n",
      "train loss:0.07250234793032453\n",
      "train loss:0.11392129240860838\n",
      "train loss:0.0916747974409468\n",
      "train loss:0.1168994778211555\n",
      "train loss:0.0648394296600096\n",
      "train loss:0.03213124485881614\n",
      "train loss:0.07933429848244691\n",
      "train loss:0.16029974599958116\n",
      "train loss:0.12764092703743368\n",
      "train loss:0.06479885342814472\n",
      "train loss:0.06238635762117743\n",
      "train loss:0.11054330674304674\n",
      "train loss:0.08543314446745212\n",
      "train loss:0.058142986946893495\n",
      "train loss:0.07762007779805936\n",
      "train loss:0.12969918314490883\n",
      "train loss:0.1341612657601125\n",
      "train loss:0.08453852397764967\n",
      "train loss:0.12094049923246714\n",
      "train loss:0.16562424919509589\n",
      "train loss:0.0813657506502399\n",
      "train loss:0.08027451781412401\n",
      "train loss:0.09855108509809858\n",
      "train loss:0.23838611825241912\n",
      "train loss:0.07322109223940036\n",
      "train loss:0.09409239135959283\n",
      "train loss:0.040651811556329916\n",
      "train loss:0.11047149789763935\n",
      "=== epoch:11, train acc:0.959, test acc:0.934 ===\n",
      "train loss:0.050908332572060545\n",
      "train loss:0.08485926549430496\n",
      "train loss:0.055385945143905345\n",
      "train loss:0.12149629757570876\n",
      "train loss:0.04074031446213294\n",
      "train loss:0.09746723913557517\n",
      "train loss:0.07301156443345475\n",
      "train loss:0.043365056197143544\n",
      "train loss:0.10292618644246934\n",
      "train loss:0.04533938266285362\n",
      "train loss:0.028756852676910447\n",
      "train loss:0.11611281370315371\n",
      "train loss:0.081636225307189\n",
      "train loss:0.129089609584445\n",
      "train loss:0.10771105768328992\n",
      "train loss:0.0759406422765825\n",
      "train loss:0.11357848618469411\n",
      "train loss:0.05321173432496073\n",
      "train loss:0.05656570285407304\n",
      "train loss:0.036598427021842565\n",
      "train loss:0.06939558414489666\n",
      "train loss:0.07353892338518431\n",
      "train loss:0.08287605172697551\n",
      "train loss:0.09436744013597632\n",
      "train loss:0.11982423309196216\n",
      "train loss:0.09130651158793748\n",
      "train loss:0.10846884375021633\n",
      "train loss:0.03997483338701621\n",
      "train loss:0.07129073294907876\n",
      "train loss:0.14360755496826827\n",
      "train loss:0.07284869909700335\n",
      "train loss:0.047679833577562725\n",
      "train loss:0.12335523060637957\n",
      "train loss:0.0796435085191336\n",
      "train loss:0.15070856822629686\n",
      "train loss:0.08728308281951519\n",
      "train loss:0.05347031485788333\n",
      "train loss:0.14352516512178096\n",
      "train loss:0.10810289460625343\n",
      "train loss:0.12243599175266233\n",
      "train loss:0.05111502042714688\n",
      "train loss:0.029837858256188433\n",
      "train loss:0.21368739853382113\n",
      "train loss:0.1262874074803695\n",
      "train loss:0.19711605472180327\n",
      "train loss:0.07511583757071866\n",
      "train loss:0.09199969407032357\n",
      "train loss:0.09996626332959123\n",
      "train loss:0.07309883259794665\n",
      "train loss:0.037361692844377216\n",
      "=== epoch:12, train acc:0.963, test acc:0.934 ===\n",
      "train loss:0.0262545108944471\n",
      "train loss:0.05548726578084672\n",
      "train loss:0.06178123794153509\n",
      "train loss:0.06407150115475985\n",
      "train loss:0.08619508190564107\n",
      "train loss:0.052135445713197705\n",
      "train loss:0.09766356086314007\n",
      "train loss:0.0709080725330206\n",
      "train loss:0.12163494920378434\n",
      "train loss:0.04125655782992413\n",
      "train loss:0.08092642459201262\n",
      "train loss:0.07423889777889014\n",
      "train loss:0.10714908057592545\n",
      "train loss:0.059596393874353344\n",
      "train loss:0.04250293386016099\n",
      "train loss:0.05702978308218306\n",
      "train loss:0.06081338389640102\n",
      "train loss:0.18401034912222816\n",
      "train loss:0.10294972780290718\n",
      "train loss:0.04862996725113731\n",
      "train loss:0.048822957517078776\n",
      "train loss:0.07509474984232854\n",
      "train loss:0.07999371695280526\n",
      "train loss:0.06072940123994942\n",
      "train loss:0.09758103638935822\n",
      "train loss:0.03149532691374264\n",
      "train loss:0.053714928612591945\n",
      "train loss:0.0755417390682332\n",
      "train loss:0.15090679584051014\n",
      "train loss:0.13997367319945328\n",
      "train loss:0.04646276618204563\n",
      "train loss:0.029209599588913483\n",
      "train loss:0.03311408329298865\n",
      "train loss:0.07368693253302473\n",
      "train loss:0.05143712111076299\n",
      "train loss:0.02895016045808422\n",
      "train loss:0.055599674117983194\n",
      "train loss:0.06453000773563485\n",
      "train loss:0.058321069932269175\n",
      "train loss:0.13285541641399368\n",
      "train loss:0.03483915478059152\n",
      "train loss:0.0585497556628737\n",
      "train loss:0.0447036877512575\n",
      "train loss:0.07041440214153205\n",
      "train loss:0.036562385004642296\n",
      "train loss:0.17855998945841076\n",
      "train loss:0.052966807117237016\n",
      "train loss:0.041785285994669845\n",
      "train loss:0.0552458684311414\n",
      "train loss:0.03563901863258218\n",
      "=== epoch:13, train acc:0.972, test acc:0.945 ===\n",
      "train loss:0.08250738899880312\n",
      "train loss:0.09736502430074269\n",
      "train loss:0.08259140222143486\n",
      "train loss:0.04498335026461445\n",
      "train loss:0.0553438216543643\n",
      "train loss:0.0650918003005594\n",
      "train loss:0.06252795105021032\n",
      "train loss:0.056391519962861694\n",
      "train loss:0.06864643815973034\n",
      "train loss:0.045818624467790905\n",
      "train loss:0.1597463964415126\n",
      "train loss:0.08434459307499008\n",
      "train loss:0.06999271764257241\n",
      "train loss:0.049201180668048565\n",
      "train loss:0.050081927220029535\n",
      "train loss:0.042488753921667434\n",
      "train loss:0.025491900089087038\n",
      "train loss:0.14656835510664448\n",
      "train loss:0.06466022001252765\n",
      "train loss:0.08257550536750202\n",
      "train loss:0.12141260770852715\n",
      "train loss:0.03808318911533065\n",
      "train loss:0.04355432804154174\n",
      "train loss:0.0776921513612592\n",
      "train loss:0.03818297191455058\n",
      "train loss:0.04419962194625457\n",
      "train loss:0.06427044613885054\n",
      "train loss:0.0430625541522299\n",
      "train loss:0.07189030109176586\n",
      "train loss:0.04166971451524646\n",
      "train loss:0.08026227793434543\n",
      "train loss:0.04993403029650409\n",
      "train loss:0.04672628952033159\n",
      "train loss:0.0770695350816113\n",
      "train loss:0.12528587773310876\n",
      "train loss:0.07247782235360133\n",
      "train loss:0.021866510204272806\n",
      "train loss:0.028722481684628206\n",
      "train loss:0.15865514532640715\n",
      "train loss:0.03333690117972096\n",
      "train loss:0.10648235092906758\n",
      "train loss:0.029594720829923274\n",
      "train loss:0.04605452597964149\n",
      "train loss:0.07314606905879038\n",
      "train loss:0.03773954428052621\n",
      "train loss:0.11367267569574988\n",
      "train loss:0.03576513097429156\n",
      "train loss:0.044568209430800886\n",
      "train loss:0.06795352611354141\n",
      "train loss:0.06372288437580577\n",
      "=== epoch:14, train acc:0.981, test acc:0.943 ===\n",
      "train loss:0.04252493788886119\n",
      "train loss:0.05779377262147421\n",
      "train loss:0.049585898963788184\n",
      "train loss:0.10567359311019947\n",
      "train loss:0.13420375167460621\n",
      "train loss:0.11109094309005639\n",
      "train loss:0.039452271820954316\n",
      "train loss:0.043581694813431146\n",
      "train loss:0.06719495200856417\n",
      "train loss:0.04271989725599009\n",
      "train loss:0.030441975089565233\n",
      "train loss:0.0673919564951549\n",
      "train loss:0.07107220172943446\n",
      "train loss:0.04257747772634077\n",
      "train loss:0.03885401412516524\n",
      "train loss:0.058515497196895756\n",
      "train loss:0.04526736793518302\n",
      "train loss:0.10342806242130254\n",
      "train loss:0.0628432243936632\n",
      "train loss:0.08969242682342987\n",
      "train loss:0.05269433335235269\n",
      "train loss:0.07896543516092783\n",
      "train loss:0.12744523518734976\n",
      "train loss:0.031644994068427294\n",
      "train loss:0.034783252947789535\n",
      "train loss:0.02884957682984028\n",
      "train loss:0.0669413959219716\n",
      "train loss:0.015502995491193135\n",
      "train loss:0.0256680028077391\n",
      "train loss:0.04338862688628953\n",
      "train loss:0.02046431492967193\n",
      "train loss:0.07261609584725509\n",
      "train loss:0.025228811582583343\n",
      "train loss:0.08339500983480354\n",
      "train loss:0.04585753167061126\n",
      "train loss:0.04096916474812238\n",
      "train loss:0.04434404991967098\n",
      "train loss:0.06656773653873542\n",
      "train loss:0.09689078994106316\n",
      "train loss:0.03042563885581487\n",
      "train loss:0.06233054783506706\n",
      "train loss:0.03305087502033664\n",
      "train loss:0.05402983103895167\n",
      "train loss:0.046558475095333736\n",
      "train loss:0.04711028857037738\n",
      "train loss:0.034437956760979854\n",
      "train loss:0.08619016111713311\n",
      "train loss:0.025816572931121322\n",
      "train loss:0.046407872071224786\n",
      "train loss:0.07609300169674879\n",
      "=== epoch:15, train acc:0.984, test acc:0.936 ===\n",
      "train loss:0.03946275526703422\n",
      "train loss:0.05643999515756543\n",
      "train loss:0.05966897522081897\n",
      "train loss:0.03410801747841397\n",
      "train loss:0.03590279107692204\n",
      "train loss:0.018667649509695985\n",
      "train loss:0.0778153670999264\n",
      "train loss:0.05160296796144431\n",
      "train loss:0.06960152419406763\n",
      "train loss:0.021028027866943515\n",
      "train loss:0.09431300877544778\n",
      "train loss:0.034253300999540014\n",
      "train loss:0.052078970339173976\n",
      "train loss:0.016116244163370246\n",
      "train loss:0.018610546278592793\n",
      "train loss:0.0214977132173398\n",
      "train loss:0.05384445953472476\n",
      "train loss:0.05654865563451684\n",
      "train loss:0.029443600558029998\n",
      "train loss:0.02080366839126018\n",
      "train loss:0.04710977119646539\n",
      "train loss:0.021696092367297172\n",
      "train loss:0.029320760616657143\n",
      "train loss:0.04024218736647354\n",
      "train loss:0.055897644798215435\n",
      "train loss:0.018473107445909846\n",
      "train loss:0.020848721443674226\n",
      "train loss:0.051373428841258305\n",
      "train loss:0.03743350318619768\n",
      "train loss:0.016054211767424845\n",
      "train loss:0.039616838233162574\n",
      "train loss:0.045762335935219535\n",
      "train loss:0.05213595590549627\n",
      "train loss:0.04709887370278368\n",
      "train loss:0.0227783896100559\n",
      "train loss:0.024150436545673823\n",
      "train loss:0.017289885794703745\n",
      "train loss:0.01827219709871051\n",
      "train loss:0.026729046405726407\n",
      "train loss:0.07999627603723573\n",
      "train loss:0.02597435767497598\n",
      "train loss:0.059762772244906684\n",
      "train loss:0.038951955449828615\n",
      "train loss:0.06757169570242379\n",
      "train loss:0.03610123769346779\n",
      "train loss:0.018216859576428948\n",
      "train loss:0.038105874797428704\n",
      "train loss:0.11648231866600019\n",
      "train loss:0.03071612020879289\n",
      "train loss:0.02583326273777008\n",
      "=== epoch:16, train acc:0.99, test acc:0.951 ===\n",
      "train loss:0.055848787820704536\n",
      "train loss:0.02550940895777938\n",
      "train loss:0.04343690581923334\n",
      "train loss:0.019941748055284876\n",
      "train loss:0.014960703819623333\n",
      "train loss:0.04577830198039955\n",
      "train loss:0.06648070966273627\n",
      "train loss:0.02975553624897698\n",
      "train loss:0.02956846079348778\n",
      "train loss:0.027019097886085173\n",
      "train loss:0.017015488730751854\n",
      "train loss:0.0201437974132969\n",
      "train loss:0.017728810463203726\n",
      "train loss:0.0194516540467452\n",
      "train loss:0.05233639205113292\n",
      "train loss:0.01973204966049212\n",
      "train loss:0.010831898732717317\n",
      "train loss:0.0551642752015846\n",
      "train loss:0.014647714270618587\n",
      "train loss:0.060573645508241646\n",
      "train loss:0.019614064340533092\n",
      "train loss:0.0402368219158393\n",
      "train loss:0.01390782060266107\n",
      "train loss:0.03147286009572537\n",
      "train loss:0.020028390199367717\n",
      "train loss:0.04432897054713204\n",
      "train loss:0.07822630783626697\n",
      "train loss:0.03066515419930345\n",
      "train loss:0.011912109663401428\n",
      "train loss:0.06870153045285522\n",
      "train loss:0.022153728447751164\n",
      "train loss:0.01565126405788455\n",
      "train loss:0.014985698629610893\n",
      "train loss:0.056987320939740196\n",
      "train loss:0.05019263089115917\n",
      "train loss:0.03751555856277823\n",
      "train loss:0.03501606990785739\n",
      "train loss:0.04225496714057035\n",
      "train loss:0.016290357618800967\n",
      "train loss:0.08837976256888574\n",
      "train loss:0.014753850619639197\n",
      "train loss:0.030729679758678196\n",
      "train loss:0.0755671006690095\n",
      "train loss:0.01579079110612858\n",
      "train loss:0.040179581122085584\n",
      "train loss:0.04751010178961615\n",
      "train loss:0.019121093954022547\n",
      "train loss:0.03993483301280052\n",
      "train loss:0.024323769860500978\n",
      "train loss:0.05040633255291989\n",
      "=== epoch:17, train acc:0.991, test acc:0.946 ===\n",
      "train loss:0.0291950228089742\n",
      "train loss:0.04236921634615541\n",
      "train loss:0.018921496601534915\n",
      "train loss:0.015124146974081366\n",
      "train loss:0.06211575569991065\n",
      "train loss:0.029349497187932706\n",
      "train loss:0.014320556919164711\n",
      "train loss:0.011863522170762081\n",
      "train loss:0.014009354778657654\n",
      "train loss:0.03950465694518363\n",
      "train loss:0.017495020371434394\n",
      "train loss:0.05615893118284099\n",
      "train loss:0.011437809314528806\n",
      "train loss:0.02726224094505607\n",
      "train loss:0.03094462045439328\n",
      "train loss:0.05404526905668539\n",
      "train loss:0.007033589320395389\n",
      "train loss:0.0179210235456751\n",
      "train loss:0.010812417961724613\n",
      "train loss:0.025787015238240767\n",
      "train loss:0.0449907212937142\n",
      "train loss:0.01577481596319014\n",
      "train loss:0.025091137731926595\n",
      "train loss:0.01828156608464141\n",
      "train loss:0.0522866959986993\n",
      "train loss:0.05775311010024493\n",
      "train loss:0.028192058306980225\n",
      "train loss:0.1259815692942132\n",
      "train loss:0.04159950529872561\n",
      "train loss:0.018203362611920394\n",
      "train loss:0.018466615132525857\n",
      "train loss:0.010246084807708826\n",
      "train loss:0.011327970414600161\n",
      "train loss:0.01774103773244478\n",
      "train loss:0.04088513518294122\n",
      "train loss:0.013035863045287597\n",
      "train loss:0.029390961104397793\n",
      "train loss:0.026738545210792303\n",
      "train loss:0.05075361994324063\n",
      "train loss:0.015266586385535985\n",
      "train loss:0.018641725792336347\n",
      "train loss:0.030201312774401353\n",
      "train loss:0.025817028887525864\n",
      "train loss:0.02523261356673413\n",
      "train loss:0.025135353834970547\n",
      "train loss:0.02656437456850723\n",
      "train loss:0.060406396074850856\n",
      "train loss:0.028408238495520807\n",
      "train loss:0.01934061020786747\n",
      "train loss:0.01565215081649239\n",
      "=== epoch:18, train acc:0.989, test acc:0.95 ===\n",
      "train loss:0.04202879149711895\n",
      "train loss:0.02192411146504688\n",
      "train loss:0.01149402347705337\n",
      "train loss:0.021145818293039412\n",
      "train loss:0.02203303329608946\n",
      "train loss:0.01779538215467563\n",
      "train loss:0.020887855494121293\n",
      "train loss:0.016801295796048984\n",
      "train loss:0.022061399418996784\n",
      "train loss:0.03860900102090226\n",
      "train loss:0.009236449707347555\n",
      "train loss:0.03586658231925614\n",
      "train loss:0.02540267377506704\n",
      "train loss:0.030585591449382454\n",
      "train loss:0.05372081072568875\n",
      "train loss:0.016104272183127426\n",
      "train loss:0.12364144680339571\n",
      "train loss:0.008643291215877105\n",
      "train loss:0.011457589400514858\n",
      "train loss:0.022665669289575516\n",
      "train loss:0.0529152670790887\n",
      "train loss:0.012278277871091552\n",
      "train loss:0.04385300286909357\n",
      "train loss:0.008146980728416396\n",
      "train loss:0.012013323839399492\n",
      "train loss:0.026652472243885544\n",
      "train loss:0.015185711197143358\n",
      "train loss:0.02490156555687874\n",
      "train loss:0.05822967536721411\n",
      "train loss:0.01597980696611211\n",
      "train loss:0.026625150681660607\n",
      "train loss:0.012446556496505348\n",
      "train loss:0.043573584399685866\n",
      "train loss:0.03645850050335358\n",
      "train loss:0.04585291217438295\n",
      "train loss:0.04065539987139689\n",
      "train loss:0.013437785233749723\n",
      "train loss:0.017081429186110463\n",
      "train loss:0.010198382237509563\n",
      "train loss:0.007232084540853847\n",
      "train loss:0.03998224349937108\n",
      "train loss:0.01614790520155912\n",
      "train loss:0.0173046666827373\n",
      "train loss:0.016301861983040355\n",
      "train loss:0.018347564722005216\n",
      "train loss:0.02192237614526267\n",
      "train loss:0.020891231548472892\n",
      "train loss:0.015363703434119863\n",
      "train loss:0.020690335777930294\n",
      "train loss:0.015048870923670467\n",
      "=== epoch:19, train acc:0.995, test acc:0.95 ===\n",
      "train loss:0.0316142712196566\n",
      "train loss:0.018441244998528644\n",
      "train loss:0.008744514515175562\n",
      "train loss:0.0037955914594177275\n",
      "train loss:0.019714650840260913\n",
      "train loss:0.020858374364198976\n",
      "train loss:0.010093076259516512\n",
      "train loss:0.03473656086550351\n",
      "train loss:0.02865049519756622\n",
      "train loss:0.020779342284406067\n",
      "train loss:0.04239733752974094\n",
      "train loss:0.009680083285752187\n",
      "train loss:0.012429892103375202\n",
      "train loss:0.03156270433487804\n",
      "train loss:0.009244754036438475\n",
      "train loss:0.00918031070256445\n",
      "train loss:0.014810285274072714\n",
      "train loss:0.007313961571709091\n",
      "train loss:0.05718345645047185\n",
      "train loss:0.014494420649659424\n",
      "train loss:0.01914453387279865\n",
      "train loss:0.02557139388999523\n",
      "train loss:0.04686844151359902\n",
      "train loss:0.029074669510967738\n",
      "train loss:0.009166855688358937\n",
      "train loss:0.009541455957298637\n",
      "train loss:0.038645305653384185\n",
      "train loss:0.025328483324889857\n",
      "train loss:0.008922932269200467\n",
      "train loss:0.007692485671812774\n",
      "train loss:0.018260546845378454\n",
      "train loss:0.009648021263294894\n",
      "train loss:0.014714656830060317\n",
      "train loss:0.01731373605956312\n",
      "train loss:0.1000828156905461\n",
      "train loss:0.013706716341882846\n",
      "train loss:0.02254460669353745\n",
      "train loss:0.02820380839889426\n",
      "train loss:0.010167618805936024\n",
      "train loss:0.018222517551351354\n",
      "train loss:0.00846337423773303\n",
      "train loss:0.013810140510025513\n",
      "train loss:0.021540905254086664\n",
      "train loss:0.008441888848278149\n",
      "train loss:0.008940964128061477\n",
      "train loss:0.012689370877573802\n",
      "train loss:0.009507399395810287\n",
      "train loss:0.02371134525161561\n",
      "train loss:0.010026613856427946\n",
      "train loss:0.020194744137269575\n",
      "=== epoch:20, train acc:0.995, test acc:0.952 ===\n",
      "train loss:0.038509987346775305\n",
      "train loss:0.06858441595997551\n",
      "train loss:0.022009980225755062\n",
      "train loss:0.022591873717440265\n",
      "train loss:0.012187074841449508\n",
      "train loss:0.012538950056800922\n",
      "train loss:0.011669177861627456\n",
      "train loss:0.00854991591055927\n",
      "train loss:0.028632222851745433\n",
      "train loss:0.019707682114212734\n",
      "train loss:0.018407467705219772\n",
      "train loss:0.016278701665452512\n",
      "train loss:0.015266289116986284\n",
      "train loss:0.04508289336765523\n",
      "train loss:0.012211586954719498\n",
      "train loss:0.03219968336904068\n",
      "train loss:0.02535466527583\n",
      "train loss:0.009964878274542241\n",
      "train loss:0.012681432671815474\n",
      "train loss:0.006179836815061596\n",
      "train loss:0.005420510646726042\n",
      "train loss:0.039552858742221905\n",
      "train loss:0.012955226801906836\n",
      "train loss:0.02037902862975221\n",
      "train loss:0.006942945459444759\n",
      "train loss:0.02726004683850891\n",
      "train loss:0.013172244827545854\n",
      "train loss:0.021274103756107308\n",
      "train loss:0.01521042873552123\n",
      "train loss:0.006775366876917413\n",
      "train loss:0.015565070332188678\n",
      "train loss:0.00830810211030466\n",
      "train loss:0.006604066488169113\n",
      "train loss:0.018964309009770787\n",
      "train loss:0.0033456258119872345\n",
      "train loss:0.008413283085077052\n",
      "train loss:0.011021217680534245\n",
      "train loss:0.010803851826830033\n",
      "train loss:0.011016681510842341\n",
      "train loss:0.007481403438168316\n",
      "train loss:0.01343528437324748\n",
      "train loss:0.008356480131748369\n",
      "train loss:0.006823518049235522\n",
      "train loss:0.0364928986755502\n",
      "train loss:0.00985193531168466\n",
      "train loss:0.0070258959608941116\n",
      "train loss:0.0059739713332780265\n",
      "train loss:0.10092201984811554\n",
      "train loss:0.005781863737231088\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.955\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAG2CAYAAACDLKdOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAABTYElEQVR4nO3deXwTdf4/8NckbZLe900vLrGcclVA16tQlB+KF4gHh8p+dXFXYXWBVUR0F8RrcdUFdUVkXRGXFY/FBblduW+5BIFCOXofSc+kTT6/P6YNDb3TJJOkr+fjkUeTyWTyng51Xn7mM5+PJIQQICIiIvISKqULICIiInIkhhsiIiLyKgw3RERE5FUYboiIiMirMNwQERGRV2G4ISIiIq/CcENEREReheGGiIiIvArDDREREXkVhhsiIiLyKoqGmx9++AFjx45FfHw8JEnCV1991epntm7dioEDB0Kr1aJ79+5Yvny50+skIiIiz6FouKmoqED//v3x3nvvtWn9rKwsjBkzBrfccgsOHTqEZ555Bo8//jjWr1/v5EqJiIjIU0juMnGmJElYs2YNxo0b1+w6s2bNwtq1a3H06FHrsgceeAClpaVYt26dC6okIiIid+ejdAHtsXPnTmRkZNgsy8zMxDPPPNPsZ4xGI4xGo/W1xWJBcXExIiIiIEmSs0olIiIiBxJCoKysDPHx8VCpWr7w5FHhJjc3FzExMTbLYmJiYDAYUFVVBT8/v0afWbhwIebPn++qEomIiMiJLly4gC5durS4jkeFG3vMmTMHM2fOtL7W6/VISkrChQsXEBwcrGBlRESdk9kisP9cCQrKqxEVqMOglDCoVe7dkm62CIz6yzbkGYxNvi8BiA7W4vsZNymyL7VmC0oqTCgoN6KwwoTCMiOKKowoLDfhVG4Z9p4rcWk9/buE4J/TrnfoNg0GAxITExEUFNTquh4VbmJjY5GXl2ezLC8vD8HBwU222gCAVquFVqtttDw4OJjhhojIxdYdzcH8b48jR19tXRYXosO8sWkY3Seuw9s31Vrkk3qZCVU1ZtRaLLBYgFqLBWaLQK1FNPhpQa1ZNL3cImA2X1l+vrgSBUY1VFr/Zr+7wAi887+L6BUXDK2PClofNbS+qivPfVTQ+V55Xv++Rq2CqolAJISAvqoGBWVG+VF+1c+6R2G5EUUVJrTUg7alup3B1y/AaefYtnQp8ahwM2zYMHz33Xc2yzZs2IBhw4YpVBEREbXVuqM5ePLTA7j6HJyrr8aTnx7AkocHNhlwLBaBksq6VokyEwrKq21O7g1P9iWVNa7ZmWYs237Ors9p1HUhqC78WIRAUbkJJrOlzdtQSUBEoBZRgVpEBV15VFTXYsWu861+/p+Pp+P6rhF21e9uFA035eXlOH36tPV1VlYWDh06hPDwcCQlJWHOnDm4dOkSVqxYAQB44okn8O677+IPf/gDHn30UWzevBlffPEF1q5dq9QuEBFRG5gtAvO/Pd4o2ACwLvvD6p9w+GIpispNNqGlsNwEs6XtN/b6qCREBGoQoPGBWiVBrZLgo5agVqngU//a5mfdcnUzy1US8g3V+O5obqvfPSQlDIFaHxhrLXUPM4w1DZ7XWmCssaC61mzT0mIyW2AyW1DWxFWvED9fRAVpERmoQVSQrlF4qX8dHqBp8pKY2SKw4UQecvXVTf7+JQCxITpc3zXC7S8PtpWi4Wbfvn245ZZbrK/r+8ZMnjwZy5cvR05ODrKzs63vp6amYu3atZgxYwbefvttdOnSBX//+9+RmZnp8tqJiJRktgjsySpGflk1ooN0GJoa7rQTk8UiUFVjRoWpFpXGup8mMyqMV/1s+P5V6+WXGW0uRTXFUF2LJVvPNvt+eICm0YldPuFrERWosy4L9fNt8jJPR5gtAgcXbW41IHz+62FtOg5CyJe85LBjbhSGACAySIuIAA10vuoO1a5WSZg3Ng1PfnoAEmBTf32l88ameU2wAdxonBtXMRgMCAkJgV6vZ58bIvJIjuy3IoRAYbkJ54sqcK6o0ubn5dJqVNYFFFf5VY9IDE0Nb9AqIYeWiEANfNXKzhhUf1kNaDogNHdZzV04u7+Ts7Xn/M1wQ0TkQZrrt9LSCdZiEcgrq8a5Qtvwcr7uZ0Ubw4skAQEaH/hr1AjQ1v3U+MBfq268/Kr3LxRVYtH6k61+x8pp12NYN/ft9+HpAcGVLX6O1p7zt0d1KCYi6sza0m/lj2uOIq/MiIvFlTYhxljbfMdUSQLiQ/yQEumP5IgApETIPxNC/RCs87WGF52vyu7BT80WgRW7zrd6WWdoarhd23eV0X3iMDIt1mMDgloluXV4dBSGGyIiD7HjdGGr/VaKK0yY9/WxRst9VBK6hPnZhJf6MNMlzA9an47162iNN/X76CwBwZMx3BBRp+SuzfPVNWZcaNDqcq6u5eVcUQUuFle1aRt94oOR3jXiSoiJCEB8qA4+CvdZGd0nDkseHtjosk6sB13WIc/AcENEnY7S/SYqjLXW/i5Xh5hcQ3WLg7G1xfNj0ty2ZcHTL+uQZ2C4ISK3Yqiuwb5zxdh1thi7zhbhVF4ZQv00jcb0aGqcjwBt6/9Js3cgubYQQr6111BVA0N1DfRVNbhUWo3zhXKIyS6WfxY0NZhJA0FaH6REBiA5wh8pEXU/I+XLR3f/bQfyPLzfCi/rkLMx3BCRogzVNdibJQeZ3VnFOHpJj6vHa8utqUauoeW+JgDgr1EjMrCZEBQoD3L24tfHmu2QKwF46Zvj6JsQigpTLcqqa2CoqoWhuqYusNRag0tzy2vMbWt2CQ/QWMNLUrh/g868AQjz92224+5LXtJvhciZeCs4EdnNnn4r+io5zOzOKsKus8U4drlxmEmJ8Ed6agSu7xYuBw1jrXUOnUZz65QbkW8woqrGdWOxtEYlAcF+vgjS+SA2WGfbiTciAEkR/gjx87V7+0pfViMPZjED53cA5XlAYAyQPBxQObczuaPwVnAiAiBfJjmeY0B5da21BSNQ62P37bwNtfUEWx9mdp0twq6sIhy7bGjUpyQ1MgDpqeG4vmsE0ruGIy6k6YlwW1IfgOT5hxpPLlhQbkR2USVKq1qfe0iSgFA/XwT7+SJY54tgPx/5p04OLPJynwbvN1jHzxcBGrVDfsfNGd0nDiN7ReHn3etRVXIJfmEJ6JV+E9Q+/E+6S3hqQDj+DbBuFmC4fGVZcDwwehGQdqdydTkBW26IvIyp1oLdWUXYcDwPG47nNbp1WOeranzZJlCHyCDboe0jA7XNDvve2kByT9zUDSazBbvOFuF4TtNh5vqudWEmNQKxITrH7Hwrdp4pwsQPd7W63spp6RjWLdIFFdmpE52k3I6n/u6PfwN8MQlo7q92/Ir21y8EYKkFaquBWqPtT7UGiOzhiMqtOEJxCxhuyBuVG2ux7WQBvj+ei80/56Osutb6nr9GjeggLQrLTSg31rawlcaCdT4N+q3IE/aFB/riwx+yoG9DC0i9rpEBSO8aYQ00McGuCTNXM1sEbmjD/EA/zrrVffutOOMk5S0qi4ELu4HsncDFffLJ1z8c8I9o8LPho26ZNlhurmuNo3/3tSbAaACq9UB1ad3Puoepsu3baY2wANteA4z65tfRBABpdwHmmqbDSnM/RTODQyYNAx5d57h9AC9LEXUK+WXV2Hg8H98fz8WO00Uwma/8RyYyUIOMa2MwMi0GI7pHWltgKk21KCwzoaC8uu5SjanRpZvCuucms0XuKFtdizMFFe2u77Zro3HXgARcnxqOaIXCzNU8fiA5i1luNWipS/S62UCvMZ5xmaQjhABKzgHZu+Qwk70LKGx9eocmqXxsQ49fWOMgpAsF1s5E8797AP95BjCbAGOZbVBp6mE0ADUODDAdZaoADn3WsW2oNYCPDvDRAroQx9RlJ7bcECmovR1yzxSU4/tjefj+eC4OXSi1udyTEuGPUb1jMSotBtclhXXoBC2EgKGqFgXl1cgvM6KwQQg6cL4Ee84Vt7qNtx8YgLsGJNhdAwDAWA4UnwVCusj/l+0gHtshN+t/wCf/r/X1rn8SiOlz5UTT2k913XOViwb5s6fPirkWyDtSF2bqHuW5jdeL7AkkXQ8kpsutEZVFcotOZdFVjxL5Z037g7vDaYPlMNDw4evfttaktijNllu0WtP7HqDL4Ab/Ntr478dHJ/8bcvK/H7bcEHmAtpxgLRaBQxdL8f2xPGw4ntuoBaV/YihGpcVgVFoMukcHOqwTqyRJCPH3RYi/L7pHB9m819Z+K9FBdrTWGMuA7N3Auf8B534ELh8ERN1dUP6R8okrskfdz7rnoUntbqXwiA651Qag4Gcg/ziQX/fz8sG2fXbXEvu+U+Vre8LS+ANBcUBwgtyvJDi+7nndMv+I9p+A29pnxVgOXNx75TLThb2Ng4jKF4i/Tg4z9YEmoJ19pWqqmgg/TYShorOA4ULr24u8BojoDuiaCCxNPbTBzm9la2soHvwokHqjc2txEbbcECmgtQ65v7mlG0oqa7DheJ7NgG++agnXd43AqN6xGHltjMs64jbk0H4r1Qb55GUNM4euhJl6uhC5Gb85aq18Monsbht6InoA2sCmP+NOnUJrqoCCkw2CzAn5oW/DibQ5ScMAbVDH+ky0hVp7JejYhJ8GzwOirpy8W+uzMuwpuYNq9k4g90jjfwvaECApvS7IXA8kDAR8239nnV3aGhAm/8f9AoLFDCzuAxhy0PRlNUk+Xs8ccevLmexQ3AKGG1JafThobQLEeoFaH9x8TRRG9Y7FzddEIVhn//gojlIfzlSwYIjqZ0SjFPkIxV5LL1igan6U32qDfDmhPszkHG58AgtNBlJuBFJGAMkjgLBkuUWn6DRQ+AtQeKrucVpeZm5htN/ghCtBpz70lJyX+0a4ukOuuQYoOtMgwNT9LMlqPmAExQHR1wLRaUBUL7lV4F+TgLLcJuqv24f2nKSavdul7rmxDCjLAQyX5CBouHzleUVB2/Zb5SPvR1AckPuTvN22Ckm60iqTdD0Qda3rLp1dzdMDgjVYAk32NvOAjugMNy1guKH2KKkwWUfNNdaaUWsRMFuE/NNc99NisS43N3zfIlBrsdi+NguUVdfgQok8AaIKFgxtEA721IUDAMi4NhqPDEvB9V3DnT5jsz0Orv8E8TvnIwZF1mV5iMDlYfNwXeZkeUG1vkGY2Q7kHGp8Ig9LAZJvAFJukANNaFLbi7CY5f4ENqGn7nlloX07pgkEhjwGSA44iQoLoL8oh5jCXwBLM3eY+YUB0b2B6F62YaapfkbucpKqNcohq2Hgufp5eW77W4Z6/T+gzz1yy0xIB/tsOZq7/O7t1WSLZQIw+lX3rrsOw00LGG6oJcUVJuypGzl319ki/Jxb5rTvylTtwTzfFYiXrnTOvSzCMb9mEtZbhjqmQ66z1P1HXkCg4YUnAQkSBHDN7fKJL+dwE2EmVQ4xKTfKLTOhic6psbK4rrWnQejJOWT7H3ZX0wTK4SWqlxxg6oNMYHT7+q54yknKXAtU5Mt1Hl3dtr5A934E9L3P+bXZy1N+983x1AEIwXDTIoYb72PPFAD1isqN2NNgXqOmwkyP6EAMTglDsM4XapUEH5UEtUoFH7XU4HWD5fWv1U0vP5Vbhr3rPsES38UA5KH669VPQ/BkzTOY8tjv3HNyQWvzfBtDQlhqXatM3aWmkC7Ora8lR1YD/36s9fW6j3TcAGQBUUBMbznIhCQ67g4YTztJeXKflat52u/eS/BuKeo02ntLb1G5EbuzirH7rNw6czKvcZjpGRNoHTl3aGo4ooK0Dq15RNcwjN38D0DYBhtAfm0RwHzNPxCVPNeh39tuxjL5ks/Vj7xjbQs2Nz4r333hTpcWAmPatt6Ip93/BKtSu3+NDSUPl/uktNZnJXm4qytrP0/73XdCDDfksZq74yhXX40nPz2AJQ8PxOCUcGvLzK6zRTiVV95oO9fEBCG9buTcoanhiAx0bJi5mvrCTrmfSjP/A6+SgFgUAd8+BcT2bXzbaMPXHfm/xWrDlcCiv1D3/PyVZVUl9m8bqGupcKNgA3jXCdbTqNTy3WhfTAKaG0Jx9KtsASGHYLghj2S2CMz/9nhLY4Xiqc8Oovbq6aYhh5nrG4SZCCeHGauaauD8dmD30ratf3il/GiJJqhtY2mYKhqHl+rS1mvwC5f7xIQmyXcxhSbJLTqbX2n9s21tJXElnmCVlXan3Om2ydvwPaTPCnkEhhtSTK3ZAmNt/cMMY02D57WWutfmJt8/nVfe6q3U9cGmV2wQrq+b12hoagTCAzSu2D35NtuiM8DpjfLj3I9AbVXbP3/NHfIIq00N3V4/bLupTH4YLtpXo194XXBJsg0woUlyqNEGNf6MxQzs+8hzWz94glVW2p3y9BDss0JOxHBDTnfkoh5vbTiJEzllDcKKfIu0sy24uw8eTE92+vdYGcuArB+uBJrSbNv3g+KAbrcAJ/8LVJWixXAw4dPm/4Pf0oR71keD93108ngxDcNLSGLzg9y1xBtaP3iCVRb7rJCTMdyQ01worsTr60/im8Otdz71VUvQ+qih9VHJD98Gz33U0Po2eO6jQmmVCZt/bn0QsdRIO07e7SGEPJLqmU3A6U3yyKqWBjNvqzXyaLHdM+RH9LXy3TLW8TLsDAc+GsAnsv1DzTuKN7R+8ARL5LUYbsjhSipMeGfzafxj1znUmOUT993XJeDh65MRrPNpFFY0Pqp2T/LYcAoAqYmB8ARUiA2Rbwt3uMpi4MxmOcyc2ST/n39D4V3lINPtNvkW6KZaR7whHLD1g4jcFMMNOUx1jRnLtmdhyZYzKDPKrRc39ojErNG90CchxKHfpVZJmDc2DV99thQvNjEQ3ss1kzBu7BMdmhkbgDwKa2WRPGT/2S3ypaZLB2DT2uIbILcAdM8Aut0KRHRr27a9IRyw9YOI3BDDDXWY2SLw7wMX8ZcNp6ydfK+NC8ac23vhVz2jnPa9o1V7kal5G+KqfiuxUjGWaN6GpBoEoEELiLlGbnWpamLG36ZmAa4sBkyNbx0HAMT0kYNM9wx5zhsfO++4YjggInI4hhuymxACW08W4NX//mwdDC8h1A+/H9UT4wYkQNXRVpOWWMzAulmQrhr+H0DdzEwC+HIa8OPiujBTDBhbmFm6JZJaHmU2ediV1png+A6VT0REzsNwQ3b56WIpFn73M3aelSdNDNb54Klbu2PSsBTofJ10WaXWBBSeBHKPAifXtT5Kbm01cHn/VQsleTJC/4gGj6tf1z38wuSfuhDHDZlPREROx3BD7ZJdVInXvz+Jb+vugNL4qDBleAp+c3M3hPo7cPyYqlIg76h8J1LuESD3JyD/5+ZnVW7OsKeAa8deCSwdHdWXiIjcHsMNtUlxhQnvbP4Fn+46jxqzgCQBdw9IwMxRPdElzN/+DQshjwVjDTFHgLwjjceHqacNkack8A8HTnzT+vZ7jpb7xBARUafBcEMtzqpdZZLvgFq61fYOqNm390Lv+HbeAVVrBAp+li8rNQwzzfWFCU0CYvrKYab+EZokXyKyzkztoaPkEhGR0zDcdHLNzao9d0wayk21eOv7U8g1yO+lxQVjzh29cGOPdtwBVVEIHFsDHPkXcGm/7QB39VS+QHQvILbflRAT01vu89Icbxgll4iInEISQjh/DHw3YjAYEBISAr1ej+DgYKXLUVRzs2pfLSHUD89lXoM7+8e37Q4oUyVw8js50JzeaBtodKF1AaZBkInsKY+4a4/j3zQxEF6C5wyER0REbdKe8zdbbjqplmbVricBmHNHr7bdAWUxA1nbgJ/+JfeFaTg+TNwAoN8EecC6+stKjuINA+EREZFDMdx0UnuyiludVVsA6JsQ2nywEUK+i+mnL4Ajq4Hy3CvvhSbJgabveCCqp+MKbwoHwiMiogYYbjqp/LKWg02L65Wcly85/fSFPO5MPb8woPc9QL/xQGI6x4YhIiJFMNx0UtFBOutzVRMTT1rqxvm1rldZDBz/Sg402TuvbEitBa65XW6l6Z5hf98ZIiIiB2G46aRig3VQqyRkYDfmNTPx5InA6zG06n/Ayi+AX75vMICeJF8G6jdBHiBP59hJMYmIiDqC4aYT+uliKR5dvhcZ2I0lvosbvR+LYizxXQyzWQf16gaXpWL6ypec+t7HuZWIiMhtMdx0MltO5mP6Pw+g2lSDP/l9Ckmg8cSTdQt8zNVAUALQf7zcMTgmzeX1EhERtRfDTSfyxd4LmLPmCMwWgWmJOYgqKGz9Q3cvBbr+yvnFEREROYhK6QLI+YQQeHvjL/jDv3+C2SJwz8AEzBoR2rYPV+Q7tTYiIiJHY8uNl6s1WzD366NYuecCAGD6Ld3w7KhrIJ0ra9sGAmOcWB0REZHjMdx4sUpTLZ767CA2/5wPlQTMv6sPHrk+WX4zqheg8ml6ricAnHiSiIg8FcONlyosN+Kx5Xtx+KIeWh8V3pl4HUb1jpXfrNYDn93fINhw4kkiIvIe7HPjhc4VVuDeJTtw+KIeYf6++Gza9VeCjakC+Od44PJBwD8CyHwVCI6z3UBwPDB+BSeeJCIij8SWGy9z6EIpHlu+F0UVJnQJ88Mnjw5Ft6hA+c2aamDlRODCLnngvUfWAHH9gfRfc+JJIiLyGgw3XmTzz3mY/s+DqKoxo09CMJZNGXJl+oRaE/DFJHnmbk0g8NC/5WADcOJJIiLyKgw3XuLzPdl4/qujMFsEftUzCn97aCACtXWH11wLfPk48Mt6wMcPeHAVkDhE2YKJiIichOHGwwkh8JeNv+Cvm34BANw7sAtevbcvfNV13aksFuDr6cDxrwG1BnjgUyDlBgUrJiIici6GGw9WY7bg+TVH8MW+iwCA397aHTNH9oQk1d3tJASwdibw0+eApAbuXy7P3E1EROTFGG48VIWxFtM/O4CtJwugkoBXxvXBQ+nJV1YQAlj/R2D/xwAk4J4PgF5jFKuXiIjIVRhuPFBBmRGPLt+LI5f00Pmq8M7EgRiZdtVIwpv/BOz6m/z8rnflmbyJiIg6AYYbD5NVWIHJy/Ygu7gS4QEa/H3yYAxMCrNd6X9vAv97Q35+xxvAdQ+7vlAiIiKFMNx4kIPZJXjsk30orjAhKdwfnzw6FKmRAbYr7VoCbHpZfj7yZWDoNNcXSkREpCCGGw9x7LIeEz/cheoaC/omhGDZlCGICtLarrR/ObButvz85jnAiKddXicREZHSGG48xDeHL6O6xoIhKWFYPnUoArRXHbrDq4Bvn5GfD/8dcNMsl9dIRETkDji3lIfIKa0GAIxMi2kcbI5/DXz1BAABDJkmX46qvx2ciIiok2G48RA5+ioAQFyIn+0bp74HVj8GCAsw4GHg9tcYbIiIqFNjuPEQl+tabuJDdVcWnt0KrHoYsNQAfe4F7vwroOIhJSKizo1nQg9gtgjkGuRwY225yd4lz/BtNgLXjAHufp8zeRMREcENws17772HlJQU6HQ6pKenY8+ePS2uv3jxYlxzzTXw8/NDYmIiZsyYgerqahdVq4yCMiPMFgG1SkJ0kBa4dAD45/1ATSXQ7Vbg/o8Bta/SZRIREbkFRcPNqlWrMHPmTMybNw8HDhxA//79kZmZifz8/CbX/+yzzzB79mzMmzcPJ06cwEcffYRVq1bhj3/8o4srd63Ldf1tYoK08Ck8AXx6D2A0AMkjgAn/BHy0rWyBiIio81A03Lz11luYNm0apk6dirS0NCxduhT+/v5YtmxZk+vv2LEDI0aMwIMPPoiUlBSMGjUKEydObLW1x9PV3yk1KLAIWHEXUFUCJAwGHlwFaPwVro6IiMi9KBZuTCYT9u/fj4yMK7NUq1QqZGRkYOfOnU1+Zvjw4di/f781zJw9exbfffcd7rjjjma/x2g0wmAw2Dw8TY6+CjEoxsv6OUBFARDbF3j434A2SOnSiIiI3I5ig/gVFhbCbDYjJsZ2wseYmBj8/PPPTX7mwQcfRGFhIW644QYIIVBbW4snnniixctSCxcuxPz58x1au6tdKq3CBPVWhNUWApE9gUe+AvxCFa6KiIjIPSneobg9tm7digULFuBvf/sbDhw4gC+//BJr167FK6+80uxn5syZA71eb31cuHDBhRU7Rk5pNVJUufKLAQ8CAZHKFkREROTGFGu5iYyMhFqtRl5ens3yvLw8xMbGNvmZuXPn4pFHHsHjjz8OAOjbty8qKirw61//Gs8//zxUTYzxotVqodV6dofbHH0VEqW6TtahycoWQ0RE5OYUa7nRaDQYNGgQNm3aZF1msViwadMmDBs2rMnPVFZWNgowarU8tosQwnnFKuyyvhqJUoH8IozhhoiIqCWKTpw5c+ZMTJ48GYMHD8bQoUOxePFiVFRUYOrUqQCASZMmISEhAQsXLgQAjB07Fm+99Rauu+46pKen4/Tp05g7dy7Gjh1rDTnexlhrhqGsDLG6EnlBaIqi9RAREbk7RcPNhAkTUFBQgBdffBG5ubkYMGAA1q1bZ+1knJ2dbdNS88ILL0CSJLzwwgu4dOkSoqKiMHbsWPz5z39WahecLk9vRIJUCAAQmkBI/uEKV0REROTeJOHN13OaYDAYEBISAr1ej+DgYKXLadWus0VY8vf38YlmERDdG/jNDqVLIiIicrn2nL896m6pzsimMzH72xAREbWK4cbNXS6tRpf6zsS8U4qIiKhVDDduji03RERE7cNw4+Yulza4DZwtN0RERK1iuHFzl0urOMYNERFROzDcuDlDaTHCpHL5BVtuiIiIWsVw48YqTbUIMV4GAFj8IgBtoMIVERERuT+GGzcm97eROxOreEmKiIioTRhu3Bj72xAREbUfw40b42zgRERE7cdw48ZsbgNnyw0REVGbMNy4MbbcEBERtR/DjRvLKa1Cl7oZwRGWomgtREREnoLhxo1VluTCXzJCQAJCuihdDhERkUdguHFTQgj4GrIBAObAWMBHq3BFREREnoHhxk3pq2oQbc4DAKh4SYqIiKjNGG7c1OXSanSpH8AvPEXZYoiIiDwIw42bku+U4mzgRERE7cVw46bk0YnrbgPnGDdERERtxnDjpi7rq9lyQ0REZAeGGzeVW1KOeKlIfsGWGyIiojZjuHFTxuKL8JXMsEg+QFCc0uUQERF5DIYbN6WuG+PGFNgFUKkVroaIiMhzMNy4IYtFIKDiovwiLEnZYoiIiDwMw40bKiw3Ig7ynVK+kakKV0NERORZGG7cUMM7pdQcnZiIiKhdGG7cUA7HuCEiIrIbw40bsh3jJkXRWoiIiDwNw40byi8uRaxUIr9gyw0REVG7MNy4IWPROQBAjdoP8I9QthgiIiIPw3DjhqRSeYyb6oAugCQpXA0REZFnYbhxQ7ryCwAAC+eUIiIiajeGGzdTY7Yg1JgDAPCNSFG2GCIiIg/EcONm8gzV6FJ3G7guqqvC1RAREXkehhs3c7n0ym3gqvAUZYshIiLyQAw3biZHX9VgjBv2uSEiImovhhs3U1BYiDCpXH7BMW6IiIjajeHGzRgLzgIAKn1CAW2QssUQERF5IIYbNyNKzgMAKgO6KFwJERGRZ2K4cTOaMnmMG3NwosKVEBEReSaGGzcTVHUJAKCOSFW4EiIiIs/EcONGqkxmRJlzAQAB0RzjhoiIyB4MN26k4W3gOoYbIiIiuzDcuJGc0ivhRgpLUbYYIiIiD8Vw40YK8y/BXzLCAgkI4d1SRERE9mC4cSNV+VkAAINPFOCjVbgaIiIiz8Rw40YsxXK4KfdPULgSIiIiz8Vw40Z8DPIYN7VBHOOGiIjIXgw3biSg8iIAzgZORETUEQw3bkIIgTBTDgBAF80B/IiIiOzFcOMmDNW1iBf5AICQuB4KV0NEROS5GG7cRE5JOeKlQgCANootN0RERPZiuHETxTnnoZHMqIEPEBSndDlEREQei+HGTVTknQEAFPvEACq1wtUQERF5LoYbN1FbdA4AUOYXr2whREREHo7hxk2o9OcBAMZAjnFDRETUEQw3bsKvQh7jBqHJyhZCRETk4Rhu3ESo8TIAQBPJO6WIiIg6guHGDQghEG3OAwAEx3VXuBoiIiLPxnDjBor0ZYhGCQAgvAsH8CMiIuoIhhs3UHjpNFSSQCV08A2KUrocIiIij8Zw4wbKc+QxbvLVMYAkKVwNERGRZ2O4cQOmwiwAgEHLMW6IiIg6SvFw89577yElJQU6nQ7p6enYs2dPi+uXlpZi+vTpiIuLg1arRc+ePfHdd9+5qFrnkOrGuKniGDdEREQd5qPkl69atQozZ87E0qVLkZ6ejsWLFyMzMxMnT55EdHR0o/VNJhNGjhyJ6OhorF69GgkJCTh//jxCQ0NdX7wDacsuAAAsIUkKV0JEROT5FA03b731FqZNm4apU6cCAJYuXYq1a9di2bJlmD17dqP1ly1bhuLiYuzYsQO+vr4AgJSUFFeW7BTB1fIYN74c44aIiKjDFLssZTKZsH//fmRkZFwpRqVCRkYGdu7c2eRnvvnmGwwbNgzTp09HTEwM+vTpgwULFsBsNjf7PUajEQaDwebhbiJrcwEAATFdFa6EiIjI8ykWbgoLC2E2mxETE2OzPCYmBrm5uU1+5uzZs1i9ejXMZjO+++47zJ07F2+++Sb+9Kc/Nfs9CxcuREhIiPWRmOhe/VpqK/UIRRkAIKJLT4WrISIi8nyKdyhuD4vFgujoaHzwwQcYNGgQJkyYgOeffx5Lly5t9jNz5syBXq+3Pi5cuODCiltXfOm0/FMEIjI8QuFqiIiIPJ9ifW4iIyOhVquRl5dnszwvLw+xsbFNfiYuLg6+vr5Qq9XWZddeey1yc3NhMpmg0WgafUar1UKr1Tq2eAcy5PyCaAB5qliEqzjGDRERUUcp1nKj0WgwaNAgbNq0ybrMYrFg06ZNGDZsWJOfGTFiBE6fPg2LxWJddurUKcTFxTUZbDxBdYE8xk2pNk7hSoiIiLyDopelZs6ciQ8//BCffPIJTpw4gSeffBIVFRXWu6cmTZqEOXPmWNd/8sknUVxcjKeffhqnTp3C2rVrsWDBAkyfPl2pXegwUSKPcVPh30XhSoiIiLyDoreCT5gwAQUFBXjxxReRm5uLAQMGYN26ddZOxtnZ2VCpruSvxMRErF+/HjNmzEC/fv2QkJCAp59+GrNmzVJqFzpMUzfGTW0wx7ghIiJyBEkIIZQuwpUMBgNCQkKg1+sRHBysdDm49OcBSKjJwsZBf0PG2IeULoeIiMgttef87VF3S3kdIRBekwMA8I/upnAxRERE3sGucLNlyxZH19E5VRbBD9WwCAmh8RzAj4iIyBHsCjejR49Gt27d8Kc//cntxo3xJMaCswCAPIQhLjxU2WKIiIi8hF3h5tKlS3jqqaewevVqdO3aFZmZmfjiiy9gMpkcXZ9X0+fIA/hdQhRC/X0VroaIiMg72BVuIiMjMWPGDBw6dAi7d+9Gz5498Zvf/Abx8fH43e9+h8OHDzu6Tq9UlS+33BT7xkGSOIAfERGRI3S4Q/HAgQMxZ84cPPXUUygvL8eyZcswaNAg3HjjjTh27JgjavRa5qJzAIByP45xQ0RE5Ch2h5uamhqsXr0ad9xxB5KTk7F+/Xq8++67yMvLw+nTp5GcnIz777/fkbV6HR9DNgCgJsi9JvMkIiLyZHYN4vfb3/4WK1euhBACjzzyCF577TX06dPH+n5AQADeeOMNxMfHO6xQb+RfeQkAIIUnK1wJERGR97Ar3Bw/fhzvvPMO7rnnnmYnpYyMjOQt4y2xmBFqygUA6KJ4GzgREZGj2BVuGk522eyGfXxw00032bP5zqEsBz6ohUmoERrDlhsiIiJHsavPzcKFC7Fs2bJGy5ctW4ZFixZ1uKhOoW7CzMsiEvHhAQoXQ0RE5D3sCjfvv/8+evXq1Wh57969sXTp0g4X1RlU1Q3gd0FEIS7ET+FqiIiIvIdd4SY3NxdxcXGNlkdFRSEnJ6fDRXUGlXlnAAB56hgEaBWdnJ2IiMir2BVuEhMTsX379kbLt2/fzjuk2qim8BwAoEyXoGwhREREXsauJoNp06bhmWeeQU1NDW699VYAcifjP/zhD/j973/v0AK9lUov97kxBnKMGyIiIkeyK9w899xzKCoqwm9+8xvrfFI6nQ6zZs3CnDlzHFqgt/KruAgAEKFJCldCRETkXewKN5IkYdGiRZg7dy5OnDgBPz8/9OjRo9kxb+gqtUYEmAoAAFqOcUNERORQHerJGhgYiCFDhjiqls5DfxEqCFQKLUIjG3fMJiIiIvvZHW727duHL774AtnZ2dZLU/W+/PLLDhfm1UrOAai7DTzUX9laiIiIvIxdd0t9/vnnGD58OE6cOIE1a9agpqYGx44dw+bNmxESEuLoGr2OqBvA74KIQkIox7ghIiJyJLvCzYIFC/CXv/wF3377LTQaDd5++238/PPPGD9+PJKS2EG2NdXWAfyiERPCfkpERESOZFe4OXPmDMaMGQMA0Gg0qKiogCRJmDFjBj744AOHFuiNTIVZAIASTRy0PmqFqyEiIvIudoWbsLAwlJWVAQASEhJw9OhRAEBpaSkqKysdV523qrssVRXQReFCiIiIvI9dHYp/9atfYcOGDejbty/uv/9+PP3009i8eTM2bNiA2267zdE1eh1tuTzGjSWEl/CIiIgcza5w8+6776K6uhoA8Pzzz8PX1xc7duzAvffeixdeeMGhBXodYzl0NSUAAJ/IVIWLISIi8j7tDje1tbX4z3/+g8zMTACASqXC7NmzHV6Y1yqVL0mViEBEhkcqXAwREZH3aXefGx8fHzzxxBPWlhtqpwa3gceF6hQuhoiIyPvY1aF46NChOHTokINL6SRKG4SbEI5xQ0RE5Gh29bn5zW9+g5kzZ+LChQsYNGgQAgICbN7v16+fQ4rzRpaSc1BBHuNmIFtuiIiIHM6ucPPAAw8AAH73u99Zl0mSBCEEJEmC2Wx2THVeyFSQBR2AS4hGdBDDDRERkaPZFW6ysrIcXUenUT/1QoVfAtQqSeFqiIiIvI9d4SY5OdnRdXQOQsC3LBsAUBvMMW6IiIicwa5ws2LFihbfnzRpkl3FeL3KIvjUyiM4q8MZboiIiJzBrnDz9NNP27yuqalBZWUlNBoN/P39GW6aU3dJKleEITqMs6cTERE5g123gpeUlNg8ysvLcfLkSdxwww1YuXKlo2v0HqXnAMi3gceH8jZwIiIiZ7Ar3DSlR48eePXVVxu16lAD1gH8ohEXwjuliIiInMFh4QaQRy++fPmyIzfpXRoM4MeWGyIiIuewq8/NN998Y/NaCIGcnBy8++67GDFihEMK80aWYnkAv4siii03RERETmJXuBk3bpzNa0mSEBUVhVtvvRVvvvmmI+rySubi81AByFXFIDxAo3Q5REREXsmucGOxWBxdh/ezmKEuuwgAqAlKgiRxAD8iIiJncGifG2pBWQ5UlhrUCDXUoQlKV0NEROS17Ao39957LxYtWtRo+WuvvYb777+/w0V5pbo7pS6LCMSGBipcDBERkfeyK9z88MMPuOOOOxotv/322/HDDz90uCivZHOnFDsTExEROYtd4aa8vBwaTeMOsb6+vjAYDB0uyivZjHHD28CJiIicxa5w07dvX6xatarR8s8//xxpaWkdLsorseWGiIjIJey6W2ru3Lm45557cObMGdx6660AgE2bNmHlypX417/+5dACvUZdy81FEY1xHMCPiIjIaewKN2PHjsVXX32FBQsWYPXq1fDz80O/fv2wceNG3HTTTY6u0StYSuQB/C5wAD8iIiKnsivcAMCYMWMwZswYR9bivWqNkMpyAAAlmjgE6XwVLoiIiMh72dXnZu/evdi9e3ej5bt378a+ffs6XJTX0V+EBIFKoYUmJEbpaoiIiLyaXeFm+vTpuHDhQqPlly5dwvTp0ztclNcpOQcAuCgiER/mr2wtREREXs6ucHP8+HEMHDiw0fLrrrsOx48f73BRXqeUt4ETERG5il3hRqvVIi8vr9HynJwc+PjY3Y3He5U0uA2cnYmJiIicyq5wM2rUKMyZMwd6vd66rLS0FH/84x8xcuRIhxXnNRqMcRPH28CJiIicyq5mljfeeAO/+tWvkJycjOuuuw4AcOjQIcTExOAf//iHQwv0Cg3GuLmNLTdEREROZVe4SUhIwE8//YR//vOfOHz4MPz8/DB16lRMnDgRvr68zflqovQ8JNSPTsyWGyIiImeyu4NMQEAAbrjhBiQlJcFkMgEA/vvf/wIA7rzzTsdU5w2M5ZAqiwDIHYpj2XJDRETkVHaFm7Nnz+Luu+/GkSNHIEkShBCQJMn6vtlsdliBHq+uv02pCIAmIBQ6X7XCBREREXk3uzoUP/3000hNTUV+fj78/f1x9OhRbNu2DYMHD8bWrVsdXKKHK2nYmZitNkRERM5mV8vNzp07sXnzZkRGRkKlUkGtVuOGG27AwoUL8bvf/Q4HDx50dJ2ei2PcEBERuZRdLTdmsxlBQUEAgMjISFy+fBkAkJycjJMnTzquOm/QoOUmgZ2JiYiInM6ulps+ffrg8OHDSE1NRXp6Ol577TVoNBp88MEH6Nq1q6Nr9GwNWm4S2ZmYiIjI6ewKNy+88AIqKioAAC+//DL+3//7f7jxxhsRERGBVatWObRAj2cd4yYKQ9lyQ0RE5HR2hZvMzEzr8+7du+Pnn39GcXExwsLCbO6a6vSEsBmdmFMvEBEROZ9dfW6aEh4ebnewee+995CSkgKdTof09HTs2bOnTZ/7/PPPIUkSxo0bZ9f3Ol1lMWAqByC33HDqBSIiIudzWLix16pVqzBz5kzMmzcPBw4cQP/+/ZGZmYn8/PwWP3fu3Dk8++yzuPHGG11UqR1KzwEA8kQoaiQNYoK0ytZDRETUCSgebt566y1MmzYNU6dORVpaGpYuXQp/f38sW7as2c+YzWY89NBDmD9/vnt3YC650pk4JlgHH7Xiv24iIiKvp+jZ1mQyYf/+/cjIyLAuU6lUyMjIwM6dO5v93Msvv4zo6Gg89thjrX6H0WiEwWCwebhMw9nA2d+GiIjIJRQNN4WFhTCbzYiJibFZHhMTg9zc3CY/8+OPP+Kjjz7Chx9+2KbvWLhwIUJCQqyPxMTEDtfdZjajE7O/DRERkSt41HWSsrIyPPLII/jwww8RGRnZps/MmTMHer3e+rhw4YKTq2ygwRg3vFOKiIjINeyeFdwRIiMjoVarkZeXZ7M8Ly8PsbGxjdY/c+YMzp07h7Fjx1qXWSwWAICPjw9OnjyJbt262XxGq9VCq1WoI2+DMW7S2HJDRETkEoq23Gg0GgwaNAibNm2yLrNYLNi0aROGDRvWaP1evXrhyJEjOHTokPVx55134pZbbsGhQ4dce8mpNRYLoJdbiTivFBERkeso2nIDADNnzsTkyZMxePBgDB06FIsXL0ZFRQWmTp0KAJg0aRISEhKwcOFC6HQ69OnTx+bzoaGhANBoueLKcgCzCbVQIUeEI54zghMREbmE4uFmwoQJKCgowIsvvojc3FwMGDAA69ats3Yyzs7OhkrlUV2DZHX9bS6LCJihZssNERGRi0hCCKF0Ea5kMBgQEhICvV6P4OBg533RoZXAV09gu7k3plrm4udXRkOl4tQURERE9mjP+dsDm0Q8RMMxbkJ1DDZEREQuwnDjLA1GJ+YAfkRERK7DcOMsNrOBs78NERGRqzDcOEuDMW7ieKcUERGRyzDcOEOtCTBcAsAxboiIiFyN4cYZ9BcACFRDiwKEIIGjExMREbkMw40z1I9xg0gAEi9LERERuRDDjTPU9bc5Z44CAF6WIiIiciGGG2docKdUgEaNYJ3iA0ETERF1Ggw3ztBwjJtQP0gSB/AjIiJyFYYbZ2g4xg07ExMREbkUw40zWMe4iUY8RycmIiJyKYYbRzOWA5WFAOrmlWJnYiIiIpdiuHG00mwAQIUUCAMCeBs4ERGRizHcOFr9GDdSNABwXikiIiIXY7hxNOsYN5EAgHi23BAREbkUw42j1bXcZHEAPyIiIkUw3DhayZXbwMP8feGnUStcEBERUefCcONoDca4YasNERGR6zHcOJIQNqMTs78NERGR6zHcOFJVCWAqAwBc5OjEREREimC4caSScwAAgzocRmh4WYqIiEgBDDeOVNffJlcVA4C3gRMRESmB4caR6vrbnLfwNnAiIiKlMNw4isUMZO8CAFTUWKCCBXGcNJOIiMjlGG4c4fg3wOI+wKn/AgDGqX7Ej9rfIe7yBoULIyIi6nwYbjrq+DfAF5MAw2WbxbFSMXxWT5bfJyIiIpdhuOkIixlYNwuAaPSW9Re7bra8HhEREbkEw01HnN/RqMXGlgAMl+T1iIiIyCUYbjqiPM+x6xEREVGHMdx0RGCMY9cjIiKiDmO46Yjk4UBwPACpmRUkIDhBXo+IiIhcguGmI1RqYPSiuhe2AUfUvx79qrweERERuQTDTUel3QmMXwEEx9kstgTFycvT7lSoMCIios7JR+kCvELanUCvMSg8tgUvr9yCIlUY/vH0M4APf71ERESuxrOvo6jUOBc0EN9YjEgM9YOKwYaIiEgRvCzlQJf11QA4YSYREZGSGG4cKKe0CgAQzwkziYiIFMNw40A59S03oWy5ISIiUgrDjYOYLQJHL+kBAMYaM8yWxvNNERERkfMx3DjAuqM5uGHRZuw7XwIAWLb9HG5YtBnrjuYoXBkREVHnw3DTQeuO5uDJTw9YL0nVy9VX48lPDzDgEBERuRjDTQeYLQLzvz2Opi5A1S+b/+1xXqIiIiJyIYabDtiTVdyoxaYhAbmT8Z6sYtcVRURE1Mkx3HRAflnzwcae9YiIiKjjGG46IDqobePZtHU9IiIi6jiGmw4YmhqOuBDdVfOBXyEBiAvRYWhquCvLIiIi6tQYbjpArZIwb2waADQKOPWv541Ng1rVXPwhIiIiR2O46aDRfeKw5OGBiL1qyoXYEB2WPDwQo/vEKVQZERFR58Spqx1gdJ84jEyLxZ6sYuSXVSM6SL4UxRYbIiIi12O4cRC1SsKwbhFKl0FERNTp8bIUEREReRWGGyIiIvIqDDdERETkVRhuiIiIyKsw3BAREZFXYbghIiIir8JwQ0RERF6F4YaIiIi8CsMNEREReRWGGyIiIvIqDDdERETkVRhuiIiIyKu4Rbh57733kJKSAp1Oh/T0dOzZs6fZdT/88EPceOONCAsLQ1hYGDIyMlpcn4iIiDoXxcPNqlWrMHPmTMybNw8HDhxA//79kZmZifz8/CbX37p1KyZOnIgtW7Zg586dSExMxKhRo3Dp0iUXV05ERETuSBJCCCULSE9Px5AhQ/Duu+8CACwWCxITE/Hb3/4Ws2fPbvXzZrMZYWFhePfddzFp0qRW1zcYDAgJCYFer0dwcHCH6yciIiLna8/5W9GWG5PJhP379yMjI8O6TKVSISMjAzt37mzTNiorK1FTU4Pw8PAm3zcajTAYDDYPIiIi8l6KhpvCwkKYzWbExMTYLI+JiUFubm6btjFr1izEx8fbBKSGFi5ciJCQEOsjMTGxw3UTERGR+1K8z01HvPrqq/j888+xZs0a6HS6JteZM2cO9Hq99XHhwgUXV0lERESu5KPkl0dGRkKtViMvL89meV5eHmJjY1v87BtvvIFXX30VGzduRL9+/ZpdT6vVQqvVOqReIiIicn+KttxoNBoMGjQImzZtsi6zWCzYtGkThg0b1uznXnvtNbzyyitYt24dBg8e7IpSiYiIyEMo2nIDADNnzsTkyZMxePBgDB06FIsXL0ZFRQWmTp0KAJg0aRISEhKwcOFCAMCiRYvw4osv4rPPPkNKSoq1b05gYCACAwMV2w8iIiJyD4qHmwkTJqCgoAAvvvgicnNzMWDAAKxbt87ayTg7Oxsq1ZUGpiVLlsBkMuG+++6z2c68efPw0ksvubJ0IiIickOKj3PjahznhoiIyPN4zDg3RERERI7GcENEREReheGGiIiIvArDDREREXkVhhsiIiLyKgw3RERE5FUYboiIiMirMNwQERGRV2G4ISIiIq/CcENEREReheGGiIiIvArDDREREXkVhhsiIiLyKj5KF0BERORNzGYzampqlC7DI2k0GqhUHW93YbghIiJyACEEcnNzUVpaqnQpHkulUiE1NRUajaZD22G4ISIicoD6YBMdHQ1/f39IkqR0SR7FYrHg8uXLyMnJQVJSUod+fww3REREHWQ2m63BJiIiQulyPFZUVBQuX76M2tpa+Pr62r0ddigmIiLqoPo+Nv7+/gpX4tnqL0eZzeYObYfhhoiIyEF4KapjHPX7Y7ghIiIir8JwQ0RE5CbMFoGdZ4rw9aFL2HmmCGaLULqkdklJScHixYuVLoMdiomIiNzBuqM5mP/tceToq63L4kJ0mDc2DaP7xDnte2+++WYMGDDAIaFk7969CAgI6HhRHcSWGyIiIoWtO5qDJz89YBNsACBXX40nPz2AdUdzFKpMHr+ntra2TetGRUW5RadqhhsiIiIHE0Kg0lTbpkdZdQ3mfXMMTV2Aql/20jfHUVZd06btCdH2S1lTpkzBtm3b8Pbbb0OSJEiShOXLl0OSJPz3v//FoEGDoNVq8eOPP+LMmTO46667EBMTg8DAQAwZMgQbN2602d7Vl6UkScLf//533H333fD390ePHj3wzTfftP8X2k68LEVERORgVTVmpL243iHbEgByDdXo+9L3bVr/+MuZ8Ne07fT+9ttv49SpU+jTpw9efvllAMCxY8cAALNnz8Ybb7yBrl27IiwsDBcuXMAdd9yBP//5z9BqtVixYgXGjh2LkydPIikpqdnvmD9/Pl577TW8/vrreOedd/DQQw/h/PnzCA8Pb1ON9mDLDRERUScVEhICjUYDf39/xMbGIjY2Fmq1GgDw8ssvY+TIkejWrRvCw8PRv39//N///R/69OmDHj164JVXXkG3bt1abYmZMmUKJk6ciO7du2PBggUoLy/Hnj17nLpfbLkhIiJyMD9fNY6/nNmmdfdkFWPKx3tbXW/51CEYmtp6a4efr7pN39uawYMH27wuLy/HSy+9hLVr1yInJwe1tbWoqqpCdnZ2i9vp16+f9XlAQACCg4ORn5/vkBqbw3BDRETkYJIktfnS0I09ohAXokOuvrrJfjcSgNgQHW7sEQW1ynWDBF5919Ozzz6LDRs24I033kD37t3h5+eH++67DyaTqcXtXD2NgiRJsFgsDq+3IV6WIiIiUpBaJWHe2DQAcpBpqP71vLFpTgs2Go2mTdMdbN++HVOmTMHdd9+Nvn37IjY2FufOnXNKTR3FcENERKSw0X3isOThgYgN0dksjw3RYcnDA506zk1KSgp2796Nc+fOobCwsNlWlR49euDLL7/EoUOHcPjwYTz44INOb4GxFy9LERERuYHRfeIwMi0We7KKkV9WjeggHYamhjv9UtSzzz6LyZMnIy0tDVVVVfj444+bXO+tt97Co48+iuHDhyMyMhKzZs2CwWBwam32kkR7boj3AgaDASEhIdDr9QgODla6HCIi8gLV1dXIyspCamoqdDpd6x+gJrX0e2zP+ZuXpYiIiMirMNwQERGRV2G4ISIiIq/CcENEREReheGGiIiIvArDDREREXkVhhsiIiLyKgw3RERE5FUYboiIiMircPoFIiIid2ExA+d3AOV5QGAMkDwcUKmVrsrjMNwQERG5g+PfAOtmAYbLV5YFxwOjFwFpdzrta2+++WYMGDAAixcvdsj2pkyZgtLSUnz11VcO2Z49eFmKiIhIace/Ab6YZBtsAMCQIy8//o0ydXkohhsiIiJHEwIwVbTtUW0A/vsHAE3NY123bN0seb22bK8d82FPmTIF27Ztw9tvvw1JkiBJEs6dO4ejR4/i9ttvR2BgIGJiYvDII4+gsLDQ+rnVq1ejb9++8PPzQ0REBDIyMlBRUYGXXnoJn3zyCb7++mvr9rZu3dqhX6U9eFmKiIjI0WoqgQXxDtqYkFt0Xk1s2+p/vAxoAtq06ttvv41Tp06hT58+ePnllwEAvr6+GDp0KB5//HH85S9/QVVVFWbNmoXx48dj8+bNyMnJwcSJE/Haa6/h7rvvRllZGf73v/9BCIFnn30WJ06cgMFgwMcffwwACA8Pt2uvO4LhhoiIqJMKCQmBRqOBv78/YmNjAQB/+tOfcN1112HBggXW9ZYtW4bExEScOnUK5eXlqK2txT333IPk5GQAQN++fa3r+vn5wWg0WrenBIYbIiIiR/P1l1tQ2uL8DuCf97W+3kOr5bun2vLdHXD48GFs2bIFgYGBjd47c+YMRo0ahdtuuw19+/ZFZmYmRo0ahfvuuw9hYWEd+l5HYrghIiJyNElq86UhdLtVvivKkIOm+91I8vvdbnXJbeHl5eUYO3YsFi1a1Oi9uLg4qNVqbNiwATt27MD333+Pd955B88//zx2796N1NRUp9fXFuxQTEREpCSVWr7dGwAgXfVm3evRrzot2Gg0GpjNZuvrgQMH4tixY0hJSUH37t1tHgEBcmCTJAkjRozA/PnzcfDgQWg0GqxZs6bJ7SmB4YaIiEhpaXcC41cAwXG2y4Pj5eVOHOcmJSUFu3fvxrlz51BYWIjp06ejuLgYEydOxN69e3HmzBmsX78eU6dOhdlsxu7du7FgwQLs27cP2dnZ+PLLL1FQUIBrr73Wur2ffvoJJ0+eRGFhIWpqapxWe3N4WYqIiMgdpN0J9Brj8hGKn332WUyePBlpaWmoqqpCVlYWtm/fjlmzZmHUqFEwGo1ITk7G6NGjoVKpEBwcjB9++AGLFy+GwWBAcnIy3nzzTdx+++0AgGnTpmHr1q0YPHgwysvLsWXLFtx8881O3YerSUK044Z4L2AwGBASEgK9Xo/g4GClyyEiIi9QXV2NrKwspKamQqfTKV2Ox2rp99ie8zcvSxEREZFXYbghIiIir8JwQ0RERF6F4YaIiIi8CsMNERGRg3Sye3QczlG/P4YbIiKiDvL19QUAVFZWKlyJZzOZTAAAtbpjt79znBsiIqIOUqvVCA0NRX5+PgDA398fknT1aMPUEovFgoKCAvj7+8PHp2PxhOGGiIjIAepnwa4PONR+KpUKSUlJHQ6GDDdEREQOIEkS4uLiEB0drciUA95Ao9FApep4jxmGGyIiIgdSq9Ud7jNCHeMWHYrfe+89pKSkQKfTIT09HXv27Glx/X/961/o1asXdDod+vbti++++85FlRIREZG7UzzcrFq1CjNnzsS8efNw4MAB9O/fH5mZmc1es9yxYwcmTpyIxx57DAcPHsS4ceMwbtw4HD161MWVExERkTtSfOLM9PR0DBkyBO+++y4Aubd0YmIifvvb32L27NmN1p8wYQIqKirwn//8x7rs+uuvx4ABA7B06dJWv48TZxIREXme9py/Fe1zYzKZsH//fsyZM8e6TKVSISMjAzt37mzyMzt37sTMmTNtlmVmZuKrr75qcn2j0Qij0Wh9rdfrAci/JCIiIvIM9efttrTJKBpuCgsLYTabERMTY7M8JiYGP//8c5Ofyc3NbXL93NzcJtdfuHAh5s+f32h5YmKinVUTERGRUsrKyhASEtLiOl5/t9ScOXNsWnosFguKi4sRERHh8AGWDAYDEhMTceHCBa+/5MV99V6daX+5r96rM+1vZ9lXIQTKysoQHx/f6rqKhpvIyEio1Wrk5eXZLM/Ly7MOhnS12NjYdq2v1Wqh1WptloWGhtpfdBsEBwd79T+whriv3qsz7S/31Xt1pv3tDPvaWotNPUXvltJoNBg0aBA2bdpkXWaxWLBp0yYMGzasyc8MGzbMZn0A2LBhQ7PrExERUeei+GWpmTNnYvLkyRg8eDCGDh2KxYsXo6KiAlOnTgUATJo0CQkJCVi4cCEA4Omnn8ZNN92EN998E2PGjMHnn3+Offv24YMPPlByN4iIiMhNKB5uJkyYgIKCArz44ovIzc3FgAEDsG7dOmun4ezsbJuhmIcPH47PPvsML7zwAv74xz+iR48e+Oqrr9CnTx+ldsFKq9Vi3rx5jS6DeSPuq/fqTPvLffVenWl/O9O+tpXi49wQEREROZLiIxQTERERORLDDREREXkVhhsiIiLyKgw3RERE5FUYbtrpvffeQ0pKCnQ6HdLT07Fnz54W1//Xv/6FXr16QafToW/fvvjuu+9cVKn9Fi5ciCFDhiAoKAjR0dEYN24cTp482eJnli9fDkmSbB46nc5FFXfMSy+91Kj2Xr16tfgZTzyuAJCSktJoXyVJwvTp05tc35OO6w8//ICxY8ciPj4ekiQ1mm9OCIEXX3wRcXFx8PPzQ0ZGBn755ZdWt9vev3lXaWl/a2pqMGvWLPTt2xcBAQGIj4/HpEmTcPny5Ra3ac/fgiu0dmynTJnSqO7Ro0e3ul13PLat7WtTf7+SJOH1119vdpvuelydieGmHVatWoWZM2di3rx5OHDgAPr374/MzEzk5+c3uf6OHTswceJEPPbYYzh48CDGjRuHcePG4ejRoy6uvH22bduG6dOnY9euXdiwYQNqamowatQoVFRUtPi54OBg5OTkWB/nz593UcUd17t3b5vaf/zxx2bX9dTjCgB79+612c8NGzYAAO6///5mP+Mpx7WiogL9+/fHe++91+T7r732Gv76179i6dKl2L17NwICApCZmYnq6upmt9nev3lXaml/KysrceDAAcydOxcHDhzAl19+iZMnT+LOO+9sdbvt+VtwldaOLQCMHj3apu6VK1e2uE13Pbat7WvDfczJycGyZcsgSRLuvffeFrfrjsfVqQS12dChQ8X06dOtr81ms4iPjxcLFy5scv3x48eLMWPG2CxLT08X//d//+fUOh0tPz9fABDbtm1rdp2PP/5YhISEuK4oB5o3b57o379/m9f3luMqhBBPP/206Natm7BYLE2+76nHFYBYs2aN9bXFYhGxsbHi9ddfty4rLS0VWq1WrFy5stnttPdvXilX729T9uzZIwCI8+fPN7tOe/8WlNDUvk6ePFncdddd7dqOJxzbthzXu+66S9x6660truMJx9XR2HLTRiaTCfv370dGRoZ1mUqlQkZGBnbu3NnkZ3bu3GmzPgBkZmY2u7670uv1AIDw8PAW1ysvL0dycjISExNx11134dixY64ozyF++eUXxMfHo2vXrnjooYeQnZ3d7LreclxNJhM+/fRTPProoy1OIuvJx7VeVlYWcnNzbY5bSEgI0tPTmz1u9vzNuzO9Xg9JklqdW689fwvuZOvWrYiOjsY111yDJ598EkVFRc2u6y3HNi8vD2vXrsVjjz3W6rqeelztxXDTRoWFhTCbzdaRk+vFxMQgNze3yc/k5ua2a313ZLFY8Mwzz2DEiBEtjgJ9zTXXYNmyZfj666/x6aefwmKxYPjw4bh48aILq7VPeno6li9fjnXr1mHJkiXIysrCjTfeiLKysibX94bjCgBfffUVSktLMWXKlGbX8eTj2lD9sWnPcbPnb95dVVdXY9asWZg4cWKLEyu292/BXYwePRorVqzApk2bsGjRImzbtg233347zGZzk+t7y7H95JNPEBQUhHvuuafF9Tz1uHaE4tMvkHubPn06jh492ur12WHDhtlMXjp8+HBce+21eP/99/HKK684u8wOuf32263P+/Xrh/T0dCQnJ+OLL75o0/8ReaqPPvoIt99+O+Lj45tdx5OPK8lqamowfvx4CCGwZMmSFtf11L+FBx54wPq8b9++6NevH7p164atW7fitttuU7Ay51q2bBkeeuihVjv5e+px7Qi23LRRZGQk1Go18vLybJbn5eUhNja2yc/Exsa2a31389RTT+E///kPtmzZgi5durTrs76+vrjuuutw+vRpJ1XnPKGhoejZs2eztXv6cQWA8+fPY+PGjXj88cfb9TlPPa71x6Y9x82ev3l3Ux9szp8/jw0bNrTYatOU1v4W3FXXrl0RGRnZbN3ecGz/97//4eTJk+3+GwY897i2B8NNG2k0GgwaNAibNm2yLrNYLNi0aZPN/9k2NGzYMJv1AWDDhg3Nru8uhBB46qmnsGbNGmzevBmpqant3obZbMaRI0cQFxfnhAqdq7y8HGfOnGm2dk89rg19/PHHiI6OxpgxY9r1OU89rqmpqYiNjbU5bgaDAbt37272uNnzN+9O6oPNL7/8go0bNyIiIqLd22jtb8FdXbx4EUVFRc3W7enHFpBbXgcNGoT+/fu3+7OeelzbRekezZ7k888/F1qtVixfvlwcP35c/PrXvxahoaEiNzdXCCHEI488ImbPnm1df/v27cLHx0e88cYb4sSJE2LevHnC19dXHDlyRKldaJMnn3xShISEiK1bt4qcnBzro7Ky0rrO1fs6f/58sX79enHmzBmxf/9+8cADDwidTieOHTumxC60y+9//3uxdetWkZWVJbZv3y4yMjJEZGSkyM/PF0J4z3GtZzabRVJSkpg1a1aj9zz5uJaVlYmDBw+KgwcPCgDirbfeEgcPHrTeHfTqq6+K0NBQ8fXXX4uffvpJ3HXXXSI1NVVUVVVZt3HrrbeKd955x/q6tb95JbW0vyaTSdx5552iS5cu4tChQzZ/x0aj0bqNq/e3tb8FpbS0r2VlZeLZZ58VO3fuFFlZWWLjxo1i4MCBokePHqK6utq6DU85tq39OxZCCL1eL/z9/cWSJUua3IanHFdnYrhpp3feeUckJSUJjUYjhg4dKnbt2mV976abbhKTJ0+2Wf+LL74QPXv2FBqNRvTu3VusXbvWxRW3H4AmHx9//LF1nav39ZlnnrH+XmJiYsQdd9whDhw44Pri7TBhwgQRFxcnNBqNSEhIEBMmTBCnT5+2vu8tx7Xe+vXrBQBx8uTJRu958nHdsmVLk/9u6/fHYrGIuXPnipiYGKHVasVtt93W6HeQnJws5s2bZ7Ospb95JbW0v1lZWc3+HW/ZssW6jav3t7W/BaW0tK+VlZVi1KhRIioqSvj6+ork5GQxbdq0RiHFU45ta/+OhRDi/fffF35+fqK0tLTJbXjKcXUmSQghnNo0RERERORC7HNDREREXoXhhoiIiLwKww0RERF5FYYbIiIi8ioMN0RERORVGG6IiIjIqzDcEBERkVdhuCGiTmfr1q2QJAmlpaVKl0JETsBwQ0RERF6F4YaIiIi8CsMNEbmcxWLBwoULkZqaCj8/P/Tv3x+rV68GcOWS0dq1a9GvXz/odDpcf/31OHr0qM02/v3vf6N3797QarVISUnBm2++afO+0WjErFmzkJiYCK1Wi+7du+Ojjz6yWWf//v0YPHgw/P39MXz4cJw8edL63uHDh3HLLbcgKCgIwcHBGDRoEPbt2+ek3wgRORLDDRG53MKFC7FixQosXboUx44dw4wZM/Dwww9j27Zt1nWee+45vPnmm9i7dy+ioqIwduxY1NTUAJBDyfjx4/HAAw/gyJEjeOmllzB37lwsX77c+vlJkyZh5cqV+Otf/4oTJ07g/fffR2BgoE0dzz//PN58803s27cPPj4+ePTRR63vPfTQQ+jSpQv27t2L/fv3Y/bs2fD19XXuL4aIHEPpmTuJqHOprq4W/v7+YseOHTbLH3vsMTFx4kTrrMiff/659b2ioiLh5+cnVq1aJYQQ4sEHHxQjR460+fxzzz0n0tLShBBCnDx5UgAQGzZsaLKG+u/YuHGjddnatWsFAFFVVSWEECIoKEgsX7684ztMRC7HlhsicqnTp0+jsrISI0eORGBgoPWxYsUKnDlzxrresGHDrM/Dw8NxzTXX4MSJEwCAEydOYMSIETbbHTFiBH755ReYzWYcOnQIarUaN910U4u19OvXz/o8Li4OAJCfnw8AmDlzJh5//HFkZGTg1VdftamNiNwbww0RuVR5eTkAYO3atTh06JD1cfz4cWu/m47y8/Nr03oNLzNJkgRA7g8EAC+99BKOHTuGMWPGYPPmzUhLS8OaNWscUh8RORfDDRG5VFpaGrRaLbKzs9G9e3ebR2JionW9Xbt2WZ+XlJTg1KlTuPbaawEA1157LbZv326z3e3bt6Nnz55Qq9Xo27cvLBaLTR8ee/Ts2RMzZszA999/j3vuuQcff/xxh7ZHRK7ho3QBRNS5BAUF4dlnn8WMGTNgsVhwww03QK/XY/v27QgODkZycjIA4OWXX0ZERARiYmLw/PPPIzIyEuPGjQMA/P73v8eQIUPwyiuvYMKECdi5cyfeffdd/O1vfwMApKSkYPLkyXj00Ufx17/+Ff3798f58+eRn5+P8ePHt1pjVVUVnnvuOdx3331ITU3FxYsXsXfvXtx7771O+70QkQMp3emHiDofi8UiFi9eLK655hrh6+sroqKiRGZmpti2bZu1s++3334revfuLTQajRg6dKg4fPiwzTZWr14t0tLShK+vr0hKShKvv/66zftVVVVixowZIi4uTmg0GtG9e3exbNkyIcSVDsUlJSXW9Q8ePCgAiKysLGE0GsUDDzwgEhMThUajEfHx8eKpp56ydjYmIvcmCSGEwvmKiMhq69atuOWWW1BSUoLQ0FClyyEiD8Q+N0RERORVGG6IiIjIq/CyFBEREXkVttwQERGRV2G4ISIiIq/CcENEREReheGGiIiIvArDDREREXkVhhsiIiLyKgw3RERE5FUYboiIiMirMNwQERGRV/n/fXkvftSV7xYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "network = SimpleConvNet(input_dim=(1,28,28),\n",
    "                        conv_param = {'filter_num': 30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "markers = {'train': 'o', 'test':'s'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='o', label='test', markevery=2)\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
