{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 어텐션\n",
    "## 어텐션의 구조\n",
    "Decoder 개선 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar: (5, 4)\n",
      "\n",
      "c: (4,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "T, H = 5, 4\n",
    "hs = np.random.randn(T, H)\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "\n",
    "ar = a.reshape(5,1).repeat(4, axis=1) ## repeat를 사용하지 않아도 broadcasting이 일어날 것이지만 눈에 잘 띄지 않음. 역전파도 수행해야 함. repeat에 대한 역전파는 각 미분 계수의 누적합으로 계산됨\n",
    "print(f\"ar: {ar.shape}\")\n",
    "print()\n",
    "\n",
    "t = hs * ar\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "print(f\"c: {c.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치용 가중합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: (10, 5, 4)\n",
      "c :(10, 4)\n"
     ]
    }
   ],
   "source": [
    "N, T, H = 10, 5, 4\n",
    "hs = np.random.randn(N, T, H)\n",
    "a = np.random.randn(N, T)\n",
    "ar = a.reshape(N, T, 1).repeat(H, axis=2)\n",
    "ar = a.reshape(N, T, 1) # 브로드캐스트를 사용하는 경우\n",
    "\n",
    "t = hs * ar\n",
    "print(f\"t: {t.shape}\")\n",
    "\n",
    "c = np.sum(t, axis=1)\n",
    "print(f\"c :{c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1).repeat(H, axis=2) ## (N, T, 1) -> (N, T, H)\n",
    "        t = hs * ar ## (N, T, H)\n",
    "        c = np.sum(t, axis=1) ## (N, T, H) -> (N, H)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "    \n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ## dc: (N, H) -> (N, 1, H) -> (N, T, H)\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs ## (N, T, H)\n",
    "        dhs = dt * ar ## (N, T, H)\n",
    "        da = np.sum(dar, axis=2) ## (N, T, H) -> (N, T)\n",
    "\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder 개선 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = softmax(x)\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H).repeat(T, axis=1) ## (N, 1, H) -> (N, T, H)\n",
    "        t = hs * hr ## (N, T, H)\n",
    "        s = np.sum(t, axis=2) ## (N, T, H) -> (N, T)\n",
    "        a = self.softmax.forward(s) ## (N, T)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "    \n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da) ## (N, T) -> (N, T)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)  ## (N, T, 1) -> (N, T, H)\n",
    "        dhs = dt * hr ## (N, T, H)\n",
    "        dhr = dt * hs ## (N, T, H)\n",
    "        dh = np.sum(dhr, axis=1) ## (N, H)\n",
    "\n",
    "        return dhs, dh ## (N, T, H), (N, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder 개선 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        ## hs: (N, T, H), h: (N, H)\n",
    "        a = self.attention_weight_layer.forward(hs, h) ## (N, T)\n",
    "        out = self.weight_sum_layer.forward(hs, a) ## (N, T, H), (N, T) -> (N, H)\n",
    "        self.attention_weight = a\n",
    "        return out ## (N, H)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        ## dout: (N, H)\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout) ## (N, H) -> (N, T, H), (N, T)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da) ## (N, T) -> (N, T, H), (N, H)\n",
    "        dhs = dhs0 + dhs1 ## (N, T, H)\n",
    "        return dhs, dh ## (N, T, H), (N, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        ## hs_enc: (N, T_enc, H)\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec) ## (N, T, H), np.zero_like를 사용하지 않아 메모리 효율성 up\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T): ## 각 타임 스텝에 대해 반복\n",
    "            layer = Attention()\n",
    "            ## hs_dec[:, t, :] -> (N, H)\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:, t, :]) ## (N, H)\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight) ## attention_weight: (N, H)\n",
    "\n",
    "        return out ## (N, T, H)\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout) ## (N, T, H)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            ## dout[:, t, :] -> (N, H)\n",
    "            dhs, dh = layer.backward(dout[:, t, :]) ## (N, T_enc, H), (N, H)\n",
    "            dhs_enc += dhs ## (N, T_enc, H)\n",
    "            dhs_dec[:, t, :] = dh\n",
    "        \n",
    "        return dhs_enc, dhs_dec ## (N, T_enc, H), (N, T, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션을 갖춘 seq2seq 구현\n",
    "Encoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from ch07 import Encoder, Seq2seq\n",
    "from common.time_layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        ## xs: (N, T)\n",
    "        xs = self.embed.forward(xs) ## (N, T, D)\n",
    "        hs = self.lstm.forward(xs) ## (N, T, H)\n",
    "        return hs\n",
    "    \n",
    "    def backward(self, dhs):\n",
    "        ## dhs: (N, T, H)\n",
    "        dout = self.lstm.backward(dhs) \n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout ## (N, T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4*H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4*H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4*H).astype('f')\n",
    "        affine_W = (rn(H+H, V) / np.sqrt(H+H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "    \n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:, -1] ## (N, T, H) -> (N, H)\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs) ## xs: (N, T) -> out: (N, T, D)\n",
    "        dec_hs = self.lstm.forward(out) ## out: (N, T, D) -> dec_hs: (N, T, H)\n",
    "        c = self.attention.forward(enc_hs, dec_hs) ## (N, T, H), (N, T, H) -> (N, T, H)\n",
    "        out = np.concatenate((c, dec_hs), axis=2) ## (N, T, H), (N, T, H) -> (N, T, 2*H)\n",
    "        score = self.affine.forward(out) ## out: (N, T, 2*H) -> score: (N, T, V)\n",
    "\n",
    "        return score ## (N, T, V)\n",
    "    \n",
    "    def backward(self, dscore):\n",
    "        ## dscore: (N, T, V)\n",
    "        dout = self.affine.backward(dscore) ## dout: (N, T, 2*H)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:] ## dc: (N, T, H), ddec_hs0: (N, T, H), context + hidden state 미분\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc) ## denc_hs: (N, T, H), ddec_hs1: (N, T, H)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1 ## ddec_hs: (N, T, H)\n",
    "        dout = self.lstm.backward(ddec_hs) ## (N, T, H) -> dout: (N, T, D)\n",
    "        dh = self.lstm.dh ## dh: (N, H)\n",
    "        denc_hs[:, -1] += dh ## denc_hs: (N, T, H)\n",
    "        self.embed.backward(dout) ## dout: (N, T, D)\n",
    "\n",
    "        return denc_hs ## (N, T, D)\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        ## enc_hs: (N, T_enc, H)\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1] ## (N, T_enc, H) -> (N, H)\n",
    "        self.lstm.set_state(h) \n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x) ## (1, 1) -> (1, 1, D)\n",
    "            dec_hs = self.lstm.forward(out) ## (1, 1, D) -> (1, 1, H)\n",
    "            c = self.attention.forward(enc_hs, dec_hs) ## (N, T_enc, H), (1, 1, H) -> (1, 1, H)\n",
    "            out = np.concatenate((c, dec_hs), axis=2) ## (1, 1, H) + (1, 1, H) -> (1, 1, 2*H)\n",
    "            score = self.affine.forward(out) ## (1, 1, 2*H) -> (1, 1, V)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten()) ## score.flatten(): (V,)\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "seq2seq 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 어텐션 평가\n",
    "어텐션을 갖춘 seq2seq의 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 3.98\n",
      "| 에폭 1 |  반복 21 / 351 | 시간 13[s] | 손실 2.43\n",
      "| 에폭 1 |  반복 41 / 351 | 시간 25[s] | 손실 1.76\n",
      "| 에폭 1 |  반복 61 / 351 | 시간 39[s] | 손실 1.58\n",
      "| 에폭 1 |  반복 81 / 351 | 시간 56[s] | 손실 1.29\n",
      "| 에폭 1 |  반복 101 / 351 | 시간 68[s] | 손실 1.14\n",
      "| 에폭 1 |  반복 121 / 351 | 시간 80[s] | 손실 1.09\n",
      "| 에폭 1 |  반복 141 / 351 | 시간 92[s] | 손실 1.05\n",
      "| 에폭 1 |  반복 161 / 351 | 시간 105[s] | 손실 1.03\n",
      "| 에폭 1 |  반복 181 / 351 | 시간 118[s] | 손실 1.02\n",
      "| 에폭 1 |  반복 201 / 351 | 시간 130[s] | 손실 1.01\n",
      "| 에폭 1 |  반복 221 / 351 | 시간 143[s] | 손실 1.00\n",
      "| 에폭 1 |  반복 241 / 351 | 시간 158[s] | 손실 0.99\n",
      "| 에폭 1 |  반복 261 / 351 | 시간 171[s] | 손실 0.97\n",
      "| 에폭 1 |  반복 281 / 351 | 시간 184[s] | 손실 0.94\n",
      "| 에폭 1 |  반복 301 / 351 | 시간 197[s] | 손실 0.91\n",
      "| 에폭 1 |  반복 321 / 351 | 시간 210[s] | 손실 0.86\n",
      "| 에폭 1 |  반복 341 / 351 | 시간 223[s] | 손실 0.79\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[91m☒\u001b[0m 1986-10-10\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[91m☒\u001b[0m 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[91m☒\u001b[0m 2006-02-22\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[91m☒\u001b[0m 1982-05-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[91m☒\u001b[0m 1982-10-05\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[91m☒\u001b[0m 2006-05-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[91m☒\u001b[0m 1988-05-22\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[91m☒\u001b[0m 1981-10-22\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[91m☒\u001b[0m 2016-11-08\n",
      "---\n",
      "val acc 1.200%\n",
      "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 0.69\n",
      "| 에폭 2 |  반복 21 / 351 | 시간 12[s] | 손실 0.66\n",
      "| 에폭 2 |  반복 41 / 351 | 시간 24[s] | 손실 0.55\n",
      "| 에폭 2 |  반복 61 / 351 | 시간 36[s] | 손실 0.41\n",
      "| 에폭 2 |  반복 81 / 351 | 시간 48[s] | 손실 0.30\n",
      "| 에폭 2 |  반복 101 / 351 | 시간 61[s] | 손실 0.19\n",
      "| 에폭 2 |  반복 121 / 351 | 시간 75[s] | 손실 0.13\n",
      "| 에폭 2 |  반복 141 / 351 | 시간 89[s] | 손실 0.08\n",
      "| 에폭 2 |  반복 161 / 351 | 시간 104[s] | 손실 0.05\n",
      "| 에폭 2 |  반복 181 / 351 | 시간 116[s] | 손실 0.03\n",
      "| 에폭 2 |  반복 201 / 351 | 시간 128[s] | 손실 0.02\n",
      "| 에폭 2 |  반복 221 / 351 | 시간 141[s] | 손실 0.02\n",
      "| 에폭 2 |  반복 241 / 351 | 시간 153[s] | 손실 0.02\n",
      "| 에폭 2 |  반복 261 / 351 | 시간 167[s] | 손실 0.01\n",
      "| 에폭 2 |  반복 281 / 351 | 시간 180[s] | 손실 0.01\n",
      "| 에폭 2 |  반복 301 / 351 | 시간 193[s] | 손실 0.01\n",
      "| 에폭 2 |  반복 321 / 351 | 시간 285[s] | 손실 0.01\n",
      "| 에폭 2 |  반복 341 / 351 | 시간 294[s] | 손실 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.800%\n",
      "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 21 / 351 | 시간 8[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 41 / 351 | 시간 18[s] | 손실 0.01\n",
      "| 에폭 3 |  반복 61 / 351 | 시간 381[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 81 / 351 | 시간 394[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 101 / 351 | 시간 406[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 121 / 351 | 시간 417[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 141 / 351 | 시간 429[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 161 / 351 | 시간 441[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 181 / 351 | 시간 457[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 201 / 351 | 시간 476[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 221 / 351 | 시간 497[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 241 / 351 | 시간 513[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 261 / 351 | 시간 530[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 281 / 351 | 시간 544[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 301 / 351 | 시간 560[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 321 / 351 | 시간 575[s] | 손실 0.00\n",
      "| 에폭 3 |  반복 341 / 351 | 시간 591[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.860%\n",
      "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 21 / 351 | 시간 16[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 41 / 351 | 시간 36[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 61 / 351 | 시간 55[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 81 / 351 | 시간 72[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 101 / 351 | 시간 88[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 121 / 351 | 시간 106[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 141 / 351 | 시간 124[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 161 / 351 | 시간 140[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 181 / 351 | 시간 158[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 201 / 351 | 시간 177[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 221 / 351 | 시간 194[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 241 / 351 | 시간 210[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 261 / 351 | 시간 224[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 281 / 351 | 시간 238[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 301 / 351 | 시간 252[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 321 / 351 | 시간 265[s] | 손실 0.00\n",
      "| 에폭 4 |  반복 341 / 351 | 시간 278[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 99.980%\n",
      "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 21 / 351 | 시간 18[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 41 / 351 | 시간 35[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 61 / 351 | 시간 48[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 81 / 351 | 시간 63[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 101 / 351 | 시간 78[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 121 / 351 | 시간 97[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 141 / 351 | 시간 112[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 161 / 351 | 시간 128[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 181 / 351 | 시간 143[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 201 / 351 | 시간 158[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 221 / 351 | 시간 172[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 241 / 351 | 시간 188[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 261 / 351 | 시간 204[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 281 / 351 | 시간 220[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 301 / 351 | 시간 233[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 321 / 351 | 시간 247[s] | 손실 0.00\n",
      "| 에폭 5 |  반복 341 / 351 | 시간 260[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 21 / 351 | 시간 18[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 41 / 351 | 시간 40[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 61 / 351 | 시간 56[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 81 / 351 | 시간 71[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 101 / 351 | 시간 85[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 121 / 351 | 시간 99[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 141 / 351 | 시간 114[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 161 / 351 | 시간 129[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 181 / 351 | 시간 146[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 201 / 351 | 시간 163[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 221 / 351 | 시간 179[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 241 / 351 | 시간 195[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 261 / 351 | 시간 212[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 281 / 351 | 시간 227[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 301 / 351 | 시간 240[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 321 / 351 | 시간 254[s] | 손실 0.00\n",
      "| 에폭 6 |  반복 341 / 351 | 시간 268[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 21 / 351 | 시간 14[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 41 / 351 | 시간 27[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 61 / 351 | 시간 40[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 81 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 101 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 121 / 351 | 시간 81[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 141 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 161 / 351 | 시간 107[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 181 / 351 | 시간 120[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 201 / 351 | 시간 135[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 221 / 351 | 시간 149[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 241 / 351 | 시간 163[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 261 / 351 | 시간 177[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 281 / 351 | 시간 190[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 301 / 351 | 시간 204[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 321 / 351 | 시간 217[s] | 손실 0.00\n",
      "| 에폭 7 |  반복 341 / 351 | 시간 231[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 21 / 351 | 시간 14[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 41 / 351 | 시간 27[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 61 / 351 | 시간 41[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 81 / 351 | 시간 54[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 101 / 351 | 시간 68[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 121 / 351 | 시간 81[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 141 / 351 | 시간 95[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 161 / 351 | 시간 109[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 181 / 351 | 시간 123[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 201 / 351 | 시간 137[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 221 / 351 | 시간 151[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 241 / 351 | 시간 165[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 261 / 351 | 시간 178[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 281 / 351 | 시간 192[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 301 / 351 | 시간 205[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 321 / 351 | 시간 218[s] | 손실 0.00\n",
      "| 에폭 8 |  반복 341 / 351 | 시간 232[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 21 / 351 | 시간 14[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 41 / 351 | 시간 27[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 61 / 351 | 시간 40[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 81 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 101 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 121 / 351 | 시간 80[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 141 / 351 | 시간 94[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 161 / 351 | 시간 108[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 181 / 351 | 시간 121[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 201 / 351 | 시간 135[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 221 / 351 | 시간 149[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 241 / 351 | 시간 162[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 261 / 351 | 시간 176[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 281 / 351 | 시간 189[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 301 / 351 | 시간 202[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 321 / 351 | 시간 215[s] | 손실 0.00\n",
      "| 에폭 9 |  반복 341 / 351 | 시간 229[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 21 / 351 | 시간 13[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 41 / 351 | 시간 26[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 61 / 351 | 시간 39[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 81 / 351 | 시간 53[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 101 / 351 | 시간 67[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 121 / 351 | 시간 79[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 141 / 351 | 시간 92[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 161 / 351 | 시간 105[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 181 / 351 | 시간 119[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 201 / 351 | 시간 132[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 221 / 351 | 시간 145[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 241 / 351 | 시간 159[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 261 / 351 | 시간 172[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 281 / 351 | 시간 186[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 301 / 351 | 시간 202[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 321 / 351 | 시간 217[s] | 손실 0.00\n",
      "| 에폭 10 |  반복 341 / 351 | 시간 232[s] | 손실 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "\u001b[92m☑\u001b[0m 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "\u001b[92m☑\u001b[0m 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "\u001b[92m☑\u001b[0m 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "\u001b[92m☑\u001b[0m 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "\u001b[92m☑\u001b[0m 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "\u001b[92m☑\u001b[0m 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "\u001b[92m☑\u001b[0m 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "\u001b[92m☑\u001b[0m 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "\u001b[92m☑\u001b[0m 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "\u001b[92m☑\u001b[0m 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n"
     ]
    }
   ],
   "source": [
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1, batch_size=batch_size, max_grad=max_grad)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse=True)\n",
    "    \n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.arange(1,max_epoch+1), acc_list, marker='o', color='blue')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.yticks(np.arange(0.0, 1.2, 0.2))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
